PyCharm Community edition supports Jupyter notebooks in read-only mode, to get full support for local notebooks download and try PyCharm Professional now!


Try DataSpell — a dedicated IDE for data science,
with full support for local and remote notebooks


Try Datalore — an online environment
for Jupyter notebooks in the browser

Also read more about JetBrains Data Solutions on our website

Comp2 结构化数据
主要适配于临床数据以及各种组学和模态提取的结构化数据建模和刻画。典型的应用场景探究rad_score最终临床诊断的作用。

Onekey步骤
数据校验，检查数据格式是否正确。
查看一些统计信息，检查数据时候存在异常点。
正则化，将数据变化到服从 N~(0, 1)。
通过相关系数，例如spearman、person等筛选出特征。
构建训练集和测试集，这里使用的是随机划分，正常多中心验证，需要大家根据自己的场景构建两份数据。
通过Lasso筛选特征，选取其中的非0项作为后续模型的特征。
使用机器学习算法，例如LR、SVM、RF等进行任务学习。
模型结果可视化，例如AUC、ROC曲线，混淆矩阵等。
[1]
import os
from IPython.display import display
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
from onekey_algo import OnekeyDS as okds
import pandas as pd


os.makedirs('img', exist_ok=True)
os.makedirs('results', exist_ok=True)
os.makedirs('features', exist_ok=True)
# 设置数据目录
label_file = r'F:\graduate\tongue\xinxueguan1397\data_xuerad.csv'
feature_file = r'F:\graduate\tongue\xinxueguan1397\labelxuerad.csv'
# 对应的标签文件
# label_file = r'你自己标注数据的文件地址'
# label_file = os.path.join(okds.ct, 'label_multicenter.csv')
# 读取标签数据列名
labels = ['label']
读取数据
数据以csv格式进行存储，这里如果是其他格式，可以使用自定义函数读取出每个样本的结果。

要求label_data为一个DataFrame格式，包括ID列以及后续的labels列，可以是多列，支持Multi-Task。

[2]
feature_data = pd.read_csv(feature_file)
display(feature_data)
label_data = pd.read_csv(label_file)
label_data.head()

特征拼接
将标注数据label_data与rad_data进行合并，得到训练数据。

注意：需要删掉ID这一列

[3]
from onekey_algo.custom.utils import print_join_info
# 删掉ID这一列。
print_join_info(feature_data, label_data)
combined_data = pd.merge(feature_data, label_data, on=['ID'], how='inner')
ids = combined_data['ID']
combined_data = combined_data.drop(['ID'], axis=1)
print(combined_data[labels].value_counts())
combined_data.columns
[2025-07-30 19:12:21 - <frozen onekey_algo.custom.utils>:  60]	INFO	ID特征完全匹配！

label
0        891
1        506
dtype: int64

Index(['label', 'group', 'L_mean', 'a_mean', 'b_mean', 'H_mean', 'S_mean',
       'V_mean', 'R_mean', 'G_mean',
       ...
       'URR', 'DM', 'Hypertension', 'Hypotension', 'Hyperlipidemia', 'OP',
       'GIB', 'Renal.Anemia', 'ROD', 'Dialysis.vintage'],
      dtype='object', length=1674)
获取到数据的统计信息
count，统计样本个数。
mean、std, 对应特征的均值、方差
min, 25%, 50%, 75%, max，对应特征的最小值，25,50,75分位数，最大值。
[4]
combined_data.describe()

正则化
normalize_df 为onekey中正则化的API，将数据变化到0均值1方差。正则化的方法为

c
o
l
u
m
n
=
c
o
l
u
m
n
−
m
e
a
n
s
t
d
[5]
from onekey_algo.custom.components.comp1 import normalize_df
data = normalize_df(combined_data, not_norm=labels, group='group')
data = data.dropna(axis=1)
data.describe()

相关系数
计算相关系数的方法有3种可供选择

pearson （皮尔逊相关系数）: standard correlation coefficient

kendall (肯德尔相关性系数) : Kendall Tau correlation coefficient

spearman (斯皮尔曼相关性系数): Spearman rank correlation

三种相关系数参考：https://blog.csdn.net/zmqsdu9001/article/details/82840332

[6]
pearson_corr = data[data['group'] == 'train'][[c for c in data.columns if c not in labels]].corr('pearson')
# kendall_corr = data[[c for c in data.columns if c not in labels]].corr('kendall')
# spearman_corr = data[[c for c in data.columns if c not in labels]].corr('spearman')
相关系数可视化
通过修改变量名，可以可视化不同相关系数下的相关矩阵。

注意：当特征特别多的时候（大于100），尽量不要可视化，否则运行时间会特别长。

[7]
import seaborn as sns
import matplotlib.pyplot as plt
from onekey_algo.custom.components.comp1 import draw_matrix

if combined_data.shape[1] < 100:
    plt.figure(figsize=(50.0, 40.0))
    # 选择可视化的相关系数
    draw_matrix(pearson_corr, annot=True, cmap='YlGnBu', cbar=False)
    plt.savefig(f'img/feature_corr.svg', bbox_inches = 'tight')
聚类分析
通过修改变量名，可以可视化不同相关系数下的相聚类分析矩阵。

注意：当特征特别多的时候（大于100），尽量不要可视化，否则运行时间会特别长。

[8]
import seaborn as sns
import matplotlib.pyplot as plt

if combined_data.shape[1] < 100:
    pp = sns.clustermap(pearson_corr, linewidths=.5, figsize=(50.0, 40.0), cmap='YlGnBu')
    plt.setp(pp.ax_heatmap.get_yticklabels(), rotation=0)
    plt.savefig(f'img/feature_cluster.svg', bbox_inches = 'tight')
特征筛选 -- 相关系数
根据相关系数，对于相关性比较高的特征（一般文献取corr>0.9），两者保留其一。

def select_feature(corr, threshold: float = 0.9, keep: int = 1, topn=10, verbose=False):
    """
    * corr, 相关系数矩阵。
    * threshold，筛选的相关系数的阈值，大于阈值的两者保留其一（可以根据keep修改，可以是其二...）。默认阈值为0.9
    * keep，可以选择大于相关系数，保留几个，默认只保留一个。
    * topn, 每次去掉多少重复特征。
    * verbose，是否打印日志
    """
[9]
from onekey_algo.custom.components.comp1 import select_feature
sel_feature = select_feature(pearson_corr, threshold=0.9, topn=32, verbose=False)

sel_feature = sel_feature + labels + ['group']
sel_feature
['b_mean',
 'H_mean',
 'S_mean',
 'R_mean',
 'B_mean',
 'exponential_firstorder_Minimum',
 'exponential_firstorder_Skewness',
 'exponential_glcm_ClusterShade',
 'exponential_glcm_Correlation',
 'exponential_glcm_DifferenceVariance',
 'exponential_glcm_Idn',
 'exponential_glcm_Imc1',
 'exponential_glcm_Imc2',
 'exponential_glcm_InverseVariance',
 'exponential_glcm_JointAverage',
 'exponential_gldm_DependenceNonUniformity',
 'exponential_gldm_DependenceNonUniformityNormalized',
 'exponential_gldm_DependenceVariance',
 'exponential_gldm_GrayLevelNonUniformity',
 'exponential_gldm_LargeDependenceHighGrayLevelEmphasis',
 'exponential_gldm_LowGrayLevelEmphasis',
 'exponential_gldm_SmallDependenceHighGrayLevelEmphasis',
 'exponential_gldm_SmallDependenceLowGrayLevelEmphasis',
 'exponential_glrlm_RunEntropy',
 'exponential_glrlm_RunVariance',
 'exponential_glrlm_ShortRunHighGrayLevelEmphasis',
 'exponential_glszm_GrayLevelNonUniformity',
 'exponential_glszm_LargeAreaHighGrayLevelEmphasis',
 'exponential_glszm_SmallAreaEmphasis',
 'exponential_glszm_SmallAreaHighGrayLevelEmphasis',
 'exponential_glszm_SmallAreaLowGrayLevelEmphasis',
 'exponential_glszm_ZoneEntropy',
 'exponential_glszm_ZoneVariance',
 'exponential_ngtdm_Busyness',
 'exponential_ngtdm_Complexity',
 'exponential_ngtdm_Contrast',
 'gradient_firstorder_10Percentile',
 'gradient_firstorder_Range',
 'gradient_firstorder_Skewness',
 'gradient_glcm_ClusterShade',
 'gradient_glcm_Correlation',
 'gradient_glcm_Idmn',
 'gradient_glcm_Idn',
 'gradient_glcm_Imc1',
 'gradient_glszm_SizeZoneNonUniformity',
 'gradient_glszm_SmallAreaHighGrayLevelEmphasis',
 'gradient_glszm_ZoneEntropy',
 'gradient_glszm_ZoneVariance',
 'gradient_ngtdm_Busyness',
 'gradient_ngtdm_Complexity',
 'gradient_ngtdm_Contrast',
 'gradient_ngtdm_Strength',
 'lbp_3D_k_firstorder_Median',
 'lbp_3D_k_firstorder_Minimum',
 'lbp_3D_k_firstorder_Range',
 'lbp_3D_k_firstorder_Skewness',
 'lbp_3D_k_glcm_Correlation',
 'lbp_3D_k_glcm_Imc1',
 'lbp_3D_k_glcm_Imc2',
 'lbp_3D_k_gldm_LargeDependenceHighGrayLevelEmphasis',
 'lbp_3D_k_gldm_SmallDependenceHighGrayLevelEmphasis',
 'lbp_3D_k_glrlm_LowGrayLevelRunEmphasis',
 'lbp_3D_k_glrlm_RunEntropy',
 'lbp_3D_k_glrlm_ShortRunEmphasis',
 'lbp_3D_k_glrlm_ShortRunLowGrayLevelEmphasis',
 'lbp_3D_k_glszm_LowGrayLevelZoneEmphasis',
 'lbp_3D_k_glszm_SizeZoneNonUniformity',
 'lbp_3D_k_glszm_SmallAreaEmphasis',
 'lbp_3D_k_glszm_SmallAreaHighGrayLevelEmphasis',
 'lbp_3D_k_glszm_SmallAreaLowGrayLevelEmphasis',
 'lbp_3D_k_glszm_ZoneEntropy',
 'lbp_3D_k_glszm_ZoneVariance',
 'lbp_3D_k_ngtdm_Busyness',
 'lbp_3D_k_ngtdm_Coarseness',
 'lbp_3D_k_ngtdm_Complexity',
 'lbp_3D_k_ngtdm_Contrast',
 'lbp_3D_k_ngtdm_Strength',
 'lbp_3D_m1_firstorder_10Percentile',
 'lbp_3D_m1_firstorder_Median',
 'lbp_3D_m1_glrlm_ShortRunEmphasis',
 'lbp_3D_m1_glszm_GrayLevelNonUniformity',
 'lbp_3D_m1_glszm_HighGrayLevelZoneEmphasis',
 'lbp_3D_m1_glszm_SizeZoneNonUniformity',
 'lbp_3D_m1_glszm_SmallAreaEmphasis',
 'lbp_3D_m1_glszm_SmallAreaHighGrayLevelEmphasis',
 'lbp_3D_m1_glszm_SmallAreaLowGrayLevelEmphasis',
 'lbp_3D_m1_glszm_ZoneEntropy',
 'lbp_3D_m1_glszm_ZoneVariance',
 'lbp_3D_m1_ngtdm_Busyness',
 'lbp_3D_m2_firstorder_10Percentile',
 'lbp_3D_m2_firstorder_InterquartileRange',
 'lbp_3D_m2_firstorder_Median',
 'lbp_3D_m2_glcm_Idn',
 'lbp_3D_m2_glcm_Imc1',
 'lbp_3D_m2_gldm_SmallDependenceLowGrayLevelEmphasis',
 'lbp_3D_m2_glrlm_GrayLevelNonUniformityNormalized',
 'lbp_3D_m2_glrlm_LowGrayLevelRunEmphasis',
 'lbp_3D_m2_glrlm_ShortRunEmphasis',
 'lbp_3D_m2_glrlm_ShortRunHighGrayLevelEmphasis',
 'lbp_3D_m2_glrlm_ShortRunLowGrayLevelEmphasis',
 'lbp_3D_m2_glszm_GrayLevelVariance',
 'lbp_3D_m2_glszm_LowGrayLevelZoneEmphasis',
 'lbp_3D_m2_glszm_SizeZoneNonUniformity',
 'lbp_3D_m2_glszm_SmallAreaHighGrayLevelEmphasis',
 'lbp_3D_m2_glszm_SmallAreaLowGrayLevelEmphasis',
 'lbp_3D_m2_glszm_ZoneVariance',
 'lbp_3D_m2_ngtdm_Busyness',
 'lbp_3D_m2_ngtdm_Strength',
 'logarithm_firstorder_InterquartileRange',
 'logarithm_firstorder_Mean',
 'logarithm_firstorder_RobustMeanAbsoluteDeviation',
 'logarithm_firstorder_TotalEnergy',
 'logarithm_glcm_Correlation',
 'logarithm_glcm_Idm',
 'logarithm_glcm_Idmn',
 'logarithm_glcm_MaximumProbability',
 'logarithm_glcm_SumEntropy',
 'logarithm_gldm_SmallDependenceLowGrayLevelEmphasis',
 'logarithm_glrlm_LongRunLowGrayLevelEmphasis',
 'logarithm_glszm_LargeAreaLowGrayLevelEmphasis',
 'logarithm_glszm_SmallAreaLowGrayLevelEmphasis',
 'logarithm_glszm_ZoneVariance',
 'original_glcm_SumEntropy',
 'original_glszm_SmallAreaHighGrayLevelEmphasis',
 'original_ngtdm_Contrast',
 'original_shape_Elongation',
 'original_shape_Maximum2DDiameterColumn',
 'original_shape_Maximum2DDiameterRow',
 'square_firstorder_10Percentile',
 'square_firstorder_Minimum',
 'square_firstorder_RobustMeanAbsoluteDeviation',
 'square_firstorder_Skewness',
 'square_firstorder_Uniformity',
 'square_glcm_ClusterShade',
 'square_glcm_Correlation',
 'square_glcm_DifferenceEntropy',
 'square_glcm_DifferenceVariance',
 'square_glcm_Idmn',
 'square_glcm_Idn',
 'square_glcm_InverseVariance',
 'square_glcm_JointEnergy',
 'square_glcm_SumEntropy',
 'square_gldm_GrayLevelNonUniformity',
 'square_gldm_LargeDependenceHighGrayLevelEmphasis',
 'square_gldm_LowGrayLevelEmphasis',
 'square_glrlm_RunPercentage',
 'square_glrlm_RunVariance',
 'square_glszm_GrayLevelVariance',
 'square_glszm_LargeAreaHighGrayLevelEmphasis',
 'square_glszm_SmallAreaHighGrayLevelEmphasis',
 'square_glszm_SmallAreaLowGrayLevelEmphasis',
 'square_glszm_ZoneVariance',
 'square_ngtdm_Busyness',
 'square_ngtdm_Coarseness',
 'square_ngtdm_Complexity',
 'square_ngtdm_Contrast',
 'square_ngtdm_Strength',
 'squareroot_firstorder_90Percentile',
 'squareroot_firstorder_InterquartileRange',
 'squareroot_firstorder_Mean',
 'squareroot_firstorder_Median',
 'squareroot_firstorder_RootMeanSquared',
 'squareroot_glcm_ClusterProminence',
 'squareroot_glcm_DifferenceVariance',
 'squareroot_glcm_Idmn',
 'squareroot_glcm_Idn',
 'squareroot_glcm_Imc1',
 'squareroot_glcm_Imc2',
 'squareroot_glcm_InverseVariance',
 'squareroot_glcm_MaximumProbability',
 'squareroot_glcm_SumEntropy',
 'squareroot_gldm_LargeDependenceLowGrayLevelEmphasis',
 'squareroot_glrlm_LongRunHighGrayLevelEmphasis',
 'squareroot_glrlm_LongRunLowGrayLevelEmphasis',
 'squareroot_glrlm_ShortRunLowGrayLevelEmphasis',
 'squareroot_glszm_GrayLevelNonUniformity',
 'squareroot_glszm_GrayLevelNonUniformityNormalized',
 'squareroot_glszm_GrayLevelVariance',
 'squareroot_glszm_LargeAreaHighGrayLevelEmphasis',
 'squareroot_glszm_LargeAreaLowGrayLevelEmphasis',
 'squareroot_glszm_SizeZoneNonUniformity',
 'squareroot_glszm_SmallAreaHighGrayLevelEmphasis',
 'squareroot_glszm_SmallAreaLowGrayLevelEmphasis',
 'squareroot_glszm_ZoneEntropy',
 'squareroot_ngtdm_Busyness',
 'squareroot_ngtdm_Coarseness',
 'squareroot_ngtdm_Complexity',
 'squareroot_ngtdm_Contrast',
 'squareroot_ngtdm_Strength',
 'wavelet_HHH_firstorder_Mean',
 'wavelet_HHH_firstorder_Range',
 'wavelet_HHH_firstorder_Skewness',
 'wavelet_HHH_glcm_Autocorrelation',
 'wavelet_HHH_gldm_DependenceVariance',
 'wavelet_HHH_gldm_LowGrayLevelEmphasis',
 'wavelet_HHH_gldm_SmallDependenceLowGrayLevelEmphasis',
 'wavelet_HHH_glrlm_LowGrayLevelRunEmphasis',
 'wavelet_HHH_glrlm_RunEntropy',
 'wavelet_HHH_glrlm_RunVariance',
 'wavelet_HHH_glrlm_ShortRunHighGrayLevelEmphasis',
 'wavelet_HHH_glrlm_ShortRunLowGrayLevelEmphasis',
 'wavelet_HHH_glszm_GrayLevelVariance',
 'wavelet_HHH_glszm_LowGrayLevelZoneEmphasis',
 'wavelet_HHH_glszm_SizeZoneNonUniformity',
 'wavelet_HHH_glszm_SmallAreaHighGrayLevelEmphasis',
 'wavelet_HHH_glszm_SmallAreaLowGrayLevelEmphasis',
 'wavelet_HHH_glszm_ZoneEntropy',
 'wavelet_HHH_glszm_ZonePercentage',
 'wavelet_HHH_ngtdm_Contrast',
 'wavelet_HHL_firstorder_Kurtosis',
 'wavelet_HHL_firstorder_Maximum',
 'wavelet_HHL_firstorder_Mean',
 'wavelet_HHL_firstorder_Median',
 'wavelet_HHL_firstorder_Skewness',
 'wavelet_HHL_glcm_ClusterProminence',
 'wavelet_HHL_glcm_ClusterShade',
 'wavelet_HHL_glcm_Correlation',
 'wavelet_HHL_glcm_Idmn',
 'wavelet_HHL_glcm_Idn',
 'wavelet_HHL_glcm_Imc2',
 'wavelet_HHL_glszm_GrayLevelVariance',
 'wavelet_HHL_glszm_SmallAreaHighGrayLevelEmphasis',
 'wavelet_HHL_glszm_SmallAreaLowGrayLevelEmphasis',
 'wavelet_HHL_glszm_ZoneVariance',
 'wavelet_HHL_ngtdm_Busyness',
 'wavelet_HHL_ngtdm_Complexity',
 'wavelet_HHL_ngtdm_Strength',
 'wavelet_HLH_firstorder_Minimum',
 'wavelet_HLH_firstorder_Range',
 'wavelet_HLH_firstorder_Skewness',
 'wavelet_HLH_glcm_Autocorrelation',
 'wavelet_HLH_glcm_JointEntropy',
 'wavelet_HLH_gldm_LowGrayLevelEmphasis',
 'wavelet_HLH_glrlm_LowGrayLevelRunEmphasis',
 'wavelet_HLH_glrlm_RunVariance',
 'wavelet_HLH_glszm_LowGrayLevelZoneEmphasis',
 'wavelet_HLH_glszm_SmallAreaEmphasis',
 'wavelet_HLH_glszm_SmallAreaHighGrayLevelEmphasis',
 'wavelet_HLH_glszm_SmallAreaLowGrayLevelEmphasis',
 'wavelet_HLH_glszm_ZoneEntropy',
 'wavelet_HLH_glszm_ZonePercentage',
 'wavelet_HLL_firstorder_Kurtosis',
 'wavelet_HLL_firstorder_Mean',
 'wavelet_HLL_firstorder_Median',
 'wavelet_HLL_firstorder_Range',
 'wavelet_HLL_firstorder_Skewness',
 'wavelet_HLL_glcm_ClusterProminence',
 'wavelet_HLL_glcm_ClusterShade',
 'wavelet_HLL_glcm_Correlation',
 'wavelet_HLL_glcm_Idmn',
 'wavelet_HLL_glcm_Idn',
 'wavelet_HLL_glcm_Imc1',
 'wavelet_HLL_glcm_Imc2',
 'wavelet_HLL_glszm_GrayLevelVariance',
 'wavelet_HLL_glszm_LargeAreaLowGrayLevelEmphasis',
 'wavelet_HLL_glszm_SmallAreaHighGrayLevelEmphasis',
 'wavelet_HLL_glszm_SmallAreaLowGrayLevelEmphasis',
 'wavelet_HLL_ngtdm_Busyness',
 'wavelet_HLL_ngtdm_Complexity',
 'wavelet_HLL_ngtdm_Strength',
 'wavelet_LHH_firstorder_Maximum',
 'wavelet_LHH_firstorder_Minimum',
 'wavelet_LHH_firstorder_Range',
 'wavelet_LHH_firstorder_Skewness',
 'wavelet_LHH_glcm_Autocorrelation',
 'wavelet_LHH_glcm_JointEntropy',
 'wavelet_LHH_gldm_DependenceVariance',
 'wavelet_LHH_gldm_GrayLevelVariance',
 'wavelet_LHH_gldm_LowGrayLevelEmphasis',
 'wavelet_LHH_glrlm_LowGrayLevelRunEmphasis',
 'wavelet_LHH_glrlm_RunLengthNonUniformity',
 'wavelet_LHH_glszm_LowGrayLevelZoneEmphasis',
 'wavelet_LHH_glszm_SizeZoneNonUniformity',
 'wavelet_LHH_glszm_SmallAreaEmphasis',
 'wavelet_LHH_glszm_SmallAreaHighGrayLevelEmphasis',
 'wavelet_LHH_glszm_SmallAreaLowGrayLevelEmphasis',
 'wavelet_LHH_glszm_ZoneEntropy',
 'wavelet_LHH_glszm_ZonePercentage',
 'wavelet_LHH_glszm_ZoneVariance',
 'wavelet_LHH_ngtdm_Contrast',
 'wavelet_LHL_firstorder_Kurtosis',
 'wavelet_LHL_firstorder_Maximum',
 'wavelet_LHL_firstorder_Mean',
 'wavelet_LHL_firstorder_Median',
 'wavelet_LHL_firstorder_Skewness',
 'wavelet_LHL_glcm_ClusterProminence',
 'wavelet_LHL_glcm_ClusterShade',
 'wavelet_LHL_glcm_Correlation',
 'wavelet_LHL_glcm_DifferenceAverage',
 'wavelet_LHL_glcm_Idmn',
 'wavelet_LHL_glcm_Idn',
 'wavelet_LHL_glcm_Imc2',
 'wavelet_LHL_glszm_GrayLevelNonUniformity',
 'wavelet_LHL_glszm_GrayLevelVariance',
 'wavelet_LHL_glszm_LargeAreaLowGrayLevelEmphasis',
 'wavelet_LHL_glszm_SmallAreaHighGrayLevelEmphasis',
 'wavelet_LHL_glszm_SmallAreaLowGrayLevelEmphasis',
 'wavelet_LHL_glszm_ZoneVariance',
 'wavelet_LHL_ngtdm_Busyness',
 'wavelet_LHL_ngtdm_Coarseness',
 'wavelet_LHL_ngtdm_Complexity',
 'wavelet_LHL_ngtdm_Contrast',
 'wavelet_LHL_ngtdm_Strength',
 'wavelet_LLH_firstorder_90Percentile',
 'wavelet_LLH_firstorder_Maximum',
 'wavelet_LLH_firstorder_Minimum',
 'wavelet_LLH_firstorder_Range',
 'wavelet_LLH_firstorder_Skewness',
 'wavelet_LLH_glcm_Imc2',
 'wavelet_LLH_gldm_SmallDependenceHighGrayLevelEmphasis',
 'wavelet_LLH_glrlm_LongRunLowGrayLevelEmphasis',
 'wavelet_LLH_glrlm_LowGrayLevelRunEmphasis',
 'wavelet_LLH_glrlm_RunEntropy',
 'wavelet_LLH_glrlm_RunVariance',
 'wavelet_LLH_glrlm_ShortRunEmphasis',
 'wavelet_LLH_glrlm_ShortRunHighGrayLevelEmphasis',
 'wavelet_LLH_glrlm_ShortRunLowGrayLevelEmphasis',
 'wavelet_LLH_glszm_GrayLevelNonUniformity',
 'wavelet_LLH_glszm_SmallAreaEmphasis',
 'wavelet_LLH_glszm_SmallAreaHighGrayLevelEmphasis',
 'wavelet_LLH_glszm_ZoneEntropy',
 'wavelet_LLH_glszm_ZonePercentage',
 'wavelet_LLH_ngtdm_Busyness',
 'wavelet_LLH_ngtdm_Complexity',
 'wavelet_LLH_ngtdm_Contrast',
 'wavelet_LLH_ngtdm_Strength',
 'wavelet_LLL_firstorder_90Percentile',
 'wavelet_LLL_firstorder_Kurtosis',
 'wavelet_LLL_firstorder_Maximum',
 'wavelet_LLL_firstorder_Mean',
 'wavelet_LLL_firstorder_Median',
 'wavelet_LLL_firstorder_Range',
 'wavelet_LLL_firstorder_RobustMeanAbsoluteDeviation',
 'wavelet_LLL_glcm_ClusterShade',
 'wavelet_LLL_glcm_Correlation',
 'wavelet_LLL_glcm_DifferenceVariance',
 'wavelet_LLL_glcm_Idmn',
 'wavelet_LLL_glcm_Idn',
 'wavelet_LLL_glcm_Imc1',
 'wavelet_LLL_glcm_Imc2',
 'wavelet_LLL_glcm_MaximumProbability',
 'wavelet_LLL_glszm_GrayLevelNonUniformity',
 'wavelet_LLL_glszm_GrayLevelVariance',
 'wavelet_LLL_glszm_LargeAreaHighGrayLevelEmphasis',
 'wavelet_LLL_glszm_SizeZoneNonUniformity',
 'wavelet_LLL_glszm_SmallAreaHighGrayLevelEmphasis',
 'wavelet_LLL_glszm_SmallAreaLowGrayLevelEmphasis',
 'wavelet_LLL_glszm_ZoneEntropy',
 'wavelet_LLL_glszm_ZoneVariance',
 'wavelet_LLL_ngtdm_Busyness',
 'wavelet_LLL_ngtdm_Coarseness',
 'wavelet_LLL_ngtdm_Complexity',
 'wavelet_LLL_ngtdm_Contrast',
 'wavelet_LLL_ngtdm_Strength',
 'Sex',
 'Age',
 'AST',
 'Ferritin',
 'LDL_C',
 'Cr',
 'GLU',
 'HCT',
 'Ca',
 'ALT',
 'Hb',
 'Fe',
 'ALB',
 'ALP',
 'Beta2_MG',
 'K',
 'Qi_deficiency',
 'Blood_stasis',
 'Dampness_syndrome',
 '25_OH_D',
 'AG',
 'ALB.GLB',
 'APTT',
 'AST.ALT',
 'BASO',
 'BASOper',
 'BNP',
 'CK',
 'CK_MB',
 'CL',
 'CRP',
 'eGFR',
 'EOSIN',
 'EOSINper',
 'FDP',
 'FIB',
 'GGT',
 'GLB',
 'HbA1c',
 'HBcAb',
 'HBeAb',
 'HBeAg',
 'HBsAb',
 'HBsAg',
 'HDL_C',
 'hsCRP',
 'INR',
 'LDH',
 'LYM',
 'LYMper',
 'MCH',
 'MCHC',
 'MCV',
 'Mg',
 'MONO',
 'MONOper',
 'MPV',
 'Na',
 'NEUT',
 'NEUTper',
 'nonHDL_C',
 'P',
 'PA',
 'PCT',
 'PDW',
 'PLT',
 'PT',
 'PTH',
 'PTper',
 'RBC',
 'RDW',
 'RET',
 'RETper',
 'TBA',
 'TBIL',
 'TC',
 'TCO2',
 'TG',
 'TIBC',
 'TnT',
 'TP',
 'TP_Ab',
 'TSAT',
 'TSH',
 'TT',
 'UA',
 'UIBC',
 'Urea',
 'VB12',
 'WBC',
 'Kt.V',
 'URR',
 'DM',
 'Hypertension',
 'Hypotension',
 'Hyperlipidemia',
 'OP',
 'GIB',
 'Renal.Anemia',
 'ROD',
 'Dialysis.vintage',
 'label',
 'group']
过滤特征
通过sel_feature过滤出筛选出来的特征。

[10]
sel_data = data[sel_feature]


sel_data.describe()

构建数据
将样本的训练数据X与监督信息y分离出来，并且对训练数据进行划分，一般的划分原则为80%-20%

[11]
import numpy as np
import onekey_algo.custom.components as okcomp

n_classes = 2
train_data = sel_data[(sel_data['group'] == 'train')]
train_ids = ids[train_data.index]
train_data = train_data.reset_index()
train_data = train_data.drop('index', axis=1)
y_data = train_data[labels]
X_data = train_data.drop(labels + ['group'], axis=1)

test_data = sel_data[sel_data['group'] != 'train']
test_ids = ids[test_data.index]
test_data = test_data.reset_index()
test_data = test_data.drop('index', axis=1)
y_test_data = test_data[labels]
X_test_data = test_data.drop(labels + ['group'], axis=1)

y_all_data = sel_data[labels]
X_all_data = sel_data.drop(labels + ['group'], axis=1)

column_names = X_data.columns
print(f"训练集样本数：{X_data.shape}, 验证集样本数：{X_test_data.shape}")
训练集样本数：(891, 455), 验证集样本数：(506, 455)

Lasso
初始化Lasso模型，alpha为惩罚系数。具体的参数文档可以参考：文档

交叉验证
不同Lambda下的，特征的的权重大小。

def lasso_cv_coefs(X_data, y_data, points=50, column_names: List[str] = None, **kwargs):
    """

    Args:
        X_data: 训练数据
        y_data: 监督数据
        points: 打印多少个点。默认50
        column_names: 列名，默认为None，当选择的数据很多的时候，建议不要添加此参数
        **kwargs: 其他用于打印控制的参数。

    """
[12]
alpha = okcomp.comp1.lasso_cv_coefs(X_data, y_data, column_names=None, alpha_logmin=-3)
plt.savefig(f'img/feature_lasso.svg', bbox_inches = 'tight')

模型效能
def lasso_cv_efficiency(X_data, y_data, points=50, **kwargs):
    """

    Args:
        Xdata: 训练数据
        ydata: 测试数据
        points: 打印的数据密度
        **kwargs: 其他的图像样式
            # 数据点标记, fmt="o"
            # 数据点大小, ms=3
            # 数据点颜色, mfc="r"
            # 数据点边缘颜色, mec="r"
            # 误差棒颜色, ecolor="b"
            # 误差棒线宽, elinewidth=2
            # 误差棒边界线长度, capsize=2
            # 误差棒边界厚度, capthick=1
    Returns:
    """
[13]
okcomp.comp1.lasso_cv_efficiency(X_data, y_data, points=50, alpha_logmin=-3)
plt.savefig(f'img/feature_mse_label.svg', bbox_inches = 'tight')

惩罚系数
使用交叉验证的惩罚系数作为模型训练的基础。

[14]
from sklearn import linear_model

models = []
for label in labels:
    clf = linear_model.Lasso(alpha=alpha)
    clf.fit(X_data, y_data[label])
    models.append(clf)
特征筛选
筛选出其中coef > 0的特征。并且打印出相应的公式。

[15]
COEF_THRESHOLD =1e-8 # 筛选的特征阈值 1e-8
scores = []
selected_features = []
for label, model in zip(labels, models):
    feat_coef = [(feat_name, coef) for feat_name, coef in zip(column_names, model.coef_) 
                 if COEF_THRESHOLD is None or abs(coef) > COEF_THRESHOLD]
    selected_features.append([feat for feat, _ in feat_coef])
    formula = ' '.join([f"{coef:+.6f} * {feat_name}" for feat_name, coef in feat_coef])
    score = f"{label} = {model.intercept_} {'+' if formula[0] != '-' else ''} {formula}"
    scores.append(score)
    
print(scores[0])
label = 0.4332210998877621  -0.005903 * exponential_firstorder_Skewness -0.019072 * gradient_firstorder_Range -0.066375 * gradient_glcm_Idmn -0.041218 * gradient_glszm_SmallAreaHighGrayLevelEmphasis +0.014701 * lbp_3D_k_firstorder_Range -0.010086 * lbp_3D_k_glrlm_RunEntropy +0.017611 * lbp_3D_k_glrlm_ShortRunEmphasis +0.003004 * lbp_3D_m1_glrlm_ShortRunEmphasis -0.019872 * lbp_3D_m1_glszm_SmallAreaHighGrayLevelEmphasis +0.015842 * square_glcm_DifferenceVariance -0.005029 * squareroot_ngtdm_Complexity +0.102338 * wavelet_HHH_glrlm_RunVariance -0.008047 * wavelet_HHH_glszm_LowGrayLevelZoneEmphasis +0.006024 * wavelet_HHL_firstorder_Skewness -0.000636 * wavelet_HHL_glszm_ZoneVariance -0.032489 * wavelet_HLL_ngtdm_Strength +0.057939 * wavelet_LHH_glcm_JointEntropy +0.024110 * HCT -0.001723 * BASO -0.000783 * BASOper -0.000268 * HBeAb -0.009470 * hsCRP +0.010259 * Urea

特征权重
[16]
# feat_coef = sorted(feat_coef, key=lambda x: x[1])
# feat_coef_df = pd.DataFrame(feat_coef, columns=['feature_name', 'Coefficients'])
# feat_coef_df.plot(x='feature_name', y='Coefficients', kind='barh')


# plt.savefig(f'img/feature_weights.svg', bbox_inches = 'tight')

[17]
import pandas as pd
import matplotlib.pyplot as plt

# 假设 feat_coef 是一个包含特征名称和系数的列表
feat_coef = sorted(feat_coef, key=lambda x: x[1])
feat_coef_df = pd.DataFrame(feat_coef, columns=['feature_name', 'Coefficients'])

# 绘制条形图
fig, ax = plt.subplots(figsize=(15, 8))  # 调整整个图形的大小，宽10英寸，高8英寸
feat_coef_df.plot(x='feature_name', y='Coefficients', kind='barh', ax=ax)

# 调整纵轴字体大小
ax.set_yticklabels(feat_coef_df['feature_name'], fontsize=8)  # 设置纵轴字体大小为8

# 调整纵轴的范围（如果需要）
# ax.set_ylim(-0.5, len(feat_coef_df) - 0.5)  # 可以根据需要调整纵轴范围

# 保存图像
plt.savefig(f'img/feature_weights.svg', bbox_inches='tight')


进一步筛选特征
使用Lasso筛选出来的Coefficients比较高的特征作为训练数据。

[18]
X_data = X_data[selected_features[0]]

X_data.to_csv(r"D:\onekey\onekey_comp\comp2-结构化数据\features1.csv",index=False)
pd.concat([ids,X_data],axis=1 ).to_csv(r"D:\onekey\onekey_comp\comp2-结构化数据\features_ID1.csv",index=False)

X_test_data = X_test_data[selected_features[0]]
X_data.columns
Index(['exponential_firstorder_Skewness', 'gradient_firstorder_Range',
       'gradient_glcm_Idmn', 'gradient_glszm_SmallAreaHighGrayLevelEmphasis',
       'lbp_3D_k_firstorder_Range', 'lbp_3D_k_glrlm_RunEntropy',
       'lbp_3D_k_glrlm_ShortRunEmphasis', 'lbp_3D_m1_glrlm_ShortRunEmphasis',
       'lbp_3D_m1_glszm_SmallAreaHighGrayLevelEmphasis',
       'square_glcm_DifferenceVariance', 'squareroot_ngtdm_Complexity',
       'wavelet_HHH_glrlm_RunVariance',
       'wavelet_HHH_glszm_LowGrayLevelZoneEmphasis',
       'wavelet_HHL_firstorder_Skewness', 'wavelet_HHL_glszm_ZoneVariance',
       'wavelet_HLL_ngtdm_Strength', 'wavelet_LHH_glcm_JointEntropy', 'HCT',
       'BASO', 'BASOper', 'HBeAb', 'hsCRP', 'Urea'],
      dtype='object')
模型筛选
根据筛选出来的数据，做模型的初步选择。当前主要使用到的是Onekey中的

SVM，支持向量机，引用参考。
KNN，K紧邻，引用参考。
Decision Tree，决策树，引用参考。
Random Forests, 随机森林，引用参考。
XGBoost, bosting方法。引用参考。
LightGBM, bosting方法，引用参考。
[19]
# model_names = ['SVM', 'KNN', 'RandomForest', 'ExtraTrees', 'XGBoost', 'LightGBM', 'NaiveBayes', 'AdaBoost', 'GradientBoosting', 'LR', 'MLP']
model_names = ['SVM', 'XGBoost', 'LightGBM', 'AdaBoost', 'GradientBoosting', 'LR', 'MLP']

models = okcomp.comp1.create_clf_model(model_names)
model_names = list(models.keys())
交叉验证
n_trails指定随机次数，每次采用的是80%训练，随机20%进行测试，找到最好的模型，以及对应的最好的数据划分。

def get_bst_split(X_data: pd.DataFrame, y_data: pd.DataFrame,
            models: dict, test_size=0.2, metric_fn=accuracy_score, n_trails=10,
            cv: bool = False, shuffle: bool = False, metric_cut_off: float = None, random_state=None):
    """
    寻找数据集中最好的数据划分。
    Args:
        X_data: 训练数据
        y_data: 监督数据
        models: 模型名称，Dict类型、
        test_size: 测试集比例
        metric_fn: 评价模型好坏的函数，默认准确率，可选roc_auc_score。
        n_trails: 尝试多少次寻找最佳数据集划分。
        cv: 是否是交叉验证，默认是False，当为True时，n_trails为交叉验证的n_fold
        shuffle: 是否进行随机打乱
        metric_cut_off: 当metric_fn的值达到多少时进行截断。
        random_state: 随机种子

    Returns: {'max_idx': max_idx, "max_model": max_model, "max_metric": max_metric, "results": results}

    """
注意：这里采用了【挑数据】，如果想要严谨，请修改n_trails=1。

[20]
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import accuracy_score, roc_auc_score

# 随机使用n_trails次数据划分，找到最好的一次划分方法，并且保存在results中。
results = okcomp.comp1.get_bst_split(X_data, y_data, models, test_size=0.5, metric_fn=roc_auc_score, n_trails=5, cv=True, random_state=0)
_, (X_train_sel, X_test_sel, y_train_sel, y_test_sel) = results['results'][results['max_idx']]
X_train_sel, X_test_sel, y_train_sel, y_test_sel = X_data, X_test_data, y_data, y_test_data
trails, _ = zip(*results['results'])
cv_results = pd.DataFrame(trails, columns=model_names)
# 可视化每个模型在不同的数据划分中的效果。
sns.boxplot(data=cv_results)
plt.xticks(rotation=25)
plt.ylabel('AUC %')
plt.xlabel('Model Name')
plt.savefig(f'img/model_cv.svg', bbox_inches = 'tight')
[2025-07-30 19:16:06 - <frozen onekey_algo.custom.components.comp1>: 512]	WARNING	当cv=True的时候，采用的是交叉验证的模式，此时test_size的参数是不生效的，我们将忽略这个test_size设置。如果需要手动指定测试集比例，请修改cv=False。


模型筛选
使用最好的数据划分，进行后续的模型研究。

注意: 一般情况下论文使用的是随机划分的数据，但也有些论文使用【刻意】筛选的数据划分。

[21]
# import joblib
# from onekey_algo.custom.components.comp1 import plot_feature_importance, plot_learning_curve, smote_resample
# targets = []
# os.makedirs('models', exist_ok=True)
# for l in labels:
#     new_models = list(okcomp.comp1.create_clf_model(model_names).values())
#     for mn, m in zip(model_names, new_models):
#         X_train_smote, y_train_smote = X_train_sel, y_train_sel
#         # 取消下一行的注释可以使用Smote进行采样，解决样本不均衡的问题。
#         X_train_smote, y_train_smote = smote_resample(X_train_sel, y_train_sel)
#         m.fit(X_train_smote, y_train_smote[l])
#         # 保存训练的模型
#         joblib.dump(m, f'models/{mn}_{l}.pkl') 
#         # 输出模型特征重要性，只针对高级树模型有用
#         plot_feature_importance(m, selected_features[0], save_dir='img')
        
# #         plot_learning_curve(m, X_train_sel, y_train_sel, title=f'Learning Curve {mn}')
# #         plt.savefig(f"img/Rad_{mn}_learning_curve.svg", bbox_inches='tight')
#         plt.show()
#     targets.append(new_models)
[22]
# import joblib
# import json
# from onekey_algo.custom.components.comp1 import plot_feature_importance, plot_learning_curve, smote_resample
# from onekey_algo import get_param_in_cwd
# from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
# from sklearn.neural_network import MLPClassifier
# from xgboost import XGBClassifier
# from lightgbm import LGBMClassifier
# from sklearn.linear_model import LogisticRegression
# from sklearn.svm import SVC
# from sklearn.neural_network import MLPClassifier
# from onekey_algo.custom.encrypt.security import dump_onekey, load_onekey

# targets = []
# os.makedirs('models', exist_ok=True)
# for l in labels:
#     new_models = okcomp.comp1.create_clf_model_none_overfit(model_names)
#     new_models['LR'] = LogisticRegression(penalty='none', max_iter=100)
#     new_models['SVM'] = SVC(probability=True, max_iter=100, kernel='linear')
#     new_models['RandomForest'] = RandomForestClassifier(n_estimators=5, max_depth=3,
#                                                         min_samples_split=4, random_state=0)
#     new_models['XGBoost'] = XGBClassifier(n_estimators=6, objective='binary:logistic', max_depth=3, min_child_weight=.2,
#                                               use_label_encoder=False, eval_metric='error')
#     new_models['LightGBM'] = LGBMClassifier(n_estimators=2,  max_depth=1, min_child_weight=0.5,)
#     new_models['ExtraTrees'] = ExtraTreesClassifier(n_estimators=60, max_depth=5, min_samples_split=2, random_state=0)
#     new_models['GradientBoosting'] = GradientBoostingClassifier(n_estimators=50, random_state=0)
#     new_models['AdaBoost'] = AdaBoostClassifier(n_estimators=10, random_state=0)
#     new_models['MLP'] = MLPClassifier(hidden_layer_sizes=(61, 128, 64, 32), max_iter=300, solver='sgd', random_state=0)
#     model_names = list(new_models.keys())
#     new_models = list(new_models.values())
    
#     for mn, m in zip(model_names, new_models):
#         X_train_smote, y_train_smote = X_train_sel, y_train_sel
#         # 取消下一行的注释可以使用Smote进行采样，解决样本不均衡的问题。
#         if get_param_in_cwd('use_smote', False):
#             X_train_smote, y_train_smote = smote_resample(X_train_sel, y_train_sel)
#         m.fit(X_train_smote, y_train_smote[l])
#         # 输出模型特征重要性，只针对高级树模型有用
# #         plot_feature_importance(m, selected_features[0], save_dir='img')
# #         plot_learning_curve(m, X_train_sel, y_train_sel, title=f'Learning Curve {mn}')
# #         plt.savefig(f"img/Rad_{mn}_learning_curve.svg", bbox_inches='tight')
#         plt.show()
#     targets.append(new_models)
#     encoder = json.loads(open('results/norm_info.json').read())
#     dump_onekey({'TYPE':'WEBUI',
#                  'data': pd.concat([ids, combined_data[selected_features[0]]], axis=1),
#                  'models': {mn:nm for mn, nm in zip(model_names, new_models)},
#                  'encoder': {fname: encoder[fname] for fname in selected_features[0]},
#                  'feature_config': selected_features[0]},
#                 save_path=f'web/config')
[23]
import joblib
import json
from onekey_algo.custom.components.comp1 import plot_feature_importance, plot_learning_curve, smote_resample
from onekey_algo import get_param_in_cwd
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from onekey_algo.custom.encrypt.security import dump_onekey, load_onekey

targets = []
os.makedirs('models', exist_ok=True)
for l in labels:
    new_models = okcomp.comp1.create_clf_model_none_overfit(model_names)
    new_models['LR'] = LogisticRegression(penalty='l2', C=0.01, max_iter=500)
    new_models['SVM'] = SVC(probability=True, C=0.01, max_iter=500, kernel='rbf')
    new_models['RandomForest'] = RandomForestClassifier(n_estimators=30, max_depth=6, 
                                                        min_samples_split=5, min_samples_leaf=5, random_state=0)
    new_models['XGBoost'] = XGBClassifier(n_estimators=25, objective='binary:logistic', max_depth=2, 
                                          min_child_weight=20, learning_rate=0.05, gamma=0.1,
                                          use_label_encoder=False, eval_metric='error')
    new_models['LightGBM'] = LGBMClassifier(n_estimators=25, max_depth=2, learning_rate=0.5, min_child_weight=20.0)
    new_models['ExtraTrees'] = ExtraTreesClassifier(n_estimators=30, max_depth=4, min_samples_split=5, 
                                                    min_samples_leaf=5, random_state=0)
    new_models['GradientBoosting'] = GradientBoostingClassifier(n_estimators=18, learning_rate=0.1, max_depth=2, 
                                                                min_samples_split=40, min_samples_leaf=30, random_state=0)                                                            
    new_models['AdaBoost'] = AdaBoostClassifier(n_estimators=18, learning_rate=0.5, random_state=0)
    new_models['MLP'] = MLPClassifier(hidden_layer_sizes=(8,), max_iter=200, alpha=5, solver='adam', random_state=0)
    model_names = list(new_models.keys())
    new_models = list(new_models.values())
    
    for mn, m in zip(model_names, new_models):
        X_train_smote, y_train_smote = X_train_sel, y_train_sel
        # 取消下一行的注释可以使用Smote进行采样，解决样本不均衡的问题。
        if get_param_in_cwd('use_smote', False):
            X_train_smote, y_train_smote = smote_resample(X_train_sel, y_train_sel)
        m.fit(X_train_smote, y_train_smote[l])
        # 保存训练的模型
        joblib.dump(m, f'models/{mn}_{l}.pkl') 
        # 输出模型特征重要性，只针对高级树模型有用
#         plot_feature_importance(m, selected_features[0], save_dir='img')
#         plot_learning_curve(m, X_train_sel, y_train_sel, title=f'Learning Curve {mn}')
#         plt.savefig(f"img/Rad_{mn}_learning_curve.svg", bbox_inches='tight')
        plt.show()
    targets.append(new_models)
    encoder = json.loads(open('results/norm_info.json').read())
    dump_onekey({'TYPE':'WEBUI',
                 'data': pd.concat([ids, combined_data[selected_features[0]]], axis=1),
                 'models': {mn:nm for mn, nm in zip(model_names, new_models)},
                 'encoder': {fname: encoder[fname] for fname in selected_features[0]},
                 'feature_config': selected_features[0]},
                save_path=f'web/config')
预测结果
predictions，二维数据，每个label对应的每个模型的预测结果。
pred_scores，二维数据，每个label对应的每个模型的预测概率值。
[24]
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder
from onekey_algo.custom.components.delong import calc_95_CI
from onekey_algo.custom.components.metrics import analysis_pred_binary

predictions = [[(model.predict(X_train_sel), model.predict(X_test_sel)) 
                for model in target] for label, target in zip(labels, targets)]
pred_scores = [[(model.predict_proba(X_train_sel), model.predict_proba(X_test_sel)) 
                for model in target] for label, target in zip(labels, targets)]

metric = []
pred_sel_idx = []
for label, prediction, scores in zip(labels, predictions, pred_scores):
    pred_sel_idx_label = []
    for mname, (train_pred, test_pred), (train_score, test_score) in zip(model_names, prediction, scores):
        # 计算训练集指数
        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_train_sel[label], 
                                                                                              train_score[:, 1])
        ci = f"{ci[0]:.4f} - {ci[1]:.4f}"
        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f"{label}-train"))
                 
        # 计算验证集指标
        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_test_sel[label], 
                                                                                              test_score[:, 1])
        ci = f"{ci[0]:.4f} - {ci[1]:.4f}"
        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f"{label}-test"))
        # 计算thres对应的sel idx
        pred_sel_idx_label.append(np.logical_or(test_score[:, 0] >= thres, test_score[:, 1] >= thres))
    
    pred_sel_idx.append(pred_sel_idx_label)
metric = pd.DataFrame(metric, index=None, columns=['model_name', 'Accuracy', 'AUC', '95% CI',
                                                   'Sensitivity', 'Specificity', 
                                                   'PPV', 'NPV', 'Precision', 'Recall', 'F1',
                                                   'Threshold', 'Task'])
metric
# metric.to_csv(r"D:\onekey\onekey_comp\comp2-结构化数据\metric.csv",index=False)

绘制曲线
绘制的不同模型的准确率柱状图和折线图曲线。

[25]
import seaborn as sns


plt.figure(figsize=(10, 10))
plt.subplot(211)
sns.barplot(x='model_name', y='Accuracy', data=metric, hue='Task')
plt.xticks(rotation=18)
plt.subplot(212)
sns.lineplot(x='model_name', y='Accuracy', data=metric, hue='Task')
plt.xticks(rotation=18)
plt.savefig(f'img/model_acc.svg', bbox_inches = 'tight')

绘制ROC曲线
确定最好的模型，并且绘制曲线。

def draw_roc(y_test, y_score, title='ROC', labels=None):
sel_model = ['SVM', 'KNN']参数为想要绘制的模型对应的参数。

[26]
sel_model = model_names

for sm in sel_model:
    if sm in model_names:
        sel_model_idx = model_names.index(sm)
    
        # Plot all ROC curves
        plt.figure(figsize=(8, 8))
        for pred_score, label in zip(pred_scores, labels):
            okcomp.comp1.draw_roc([np.array(y_train_sel[label]), np.array(y_test_sel[label])], 
                                  pred_score[sel_model_idx], 
                                  labels=['Train', 'Test'], title=f"Model: {sm}")
            plt.savefig(f'img/model_{sm}_roc.svg', bbox_inches = 'tight')

汇总所有模型
[27]
# sel_model = model_names

# for pred_score, label in zip(pred_scores, labels):
#     pred_test_scores = []
#     for sm in sel_model:
#         if sm in model_names:
#             sel_model_idx = model_names.index(sm)
#             pred_test_scores.append(pred_score[sel_model_idx][1])
#     okcomp.comp1.draw_roc([np.array(y_test_sel[label])] * len(pred_test_scores), 
#                           pred_test_scores, 
#                           labels=sel_model, title=f"Model AUC")
#     plt.savefig(f'img/model_roc.svg', bbox_inches = 'tight')   

[28]
sel_model = model_names

for pred_score, label in zip(pred_scores, labels):
    pred_test_scores = []
    for sm in sel_model:
        if sm in model_names:
            sel_model_idx = model_names.index(sm)
            pred_test_scores.append(pred_score[sel_model_idx][0])
    okcomp.comp1.draw_roc([np.array(y_train_sel[label])] * len(pred_test_scores), 
                          pred_test_scores, 
                          labels=sel_model, title=f"Model AUC")
    plt.savefig(f'img/model_roc_train.svg', bbox_inches = 'tight')
    plt.show()

for pred_score, label in zip(pred_scores, labels):
    pred_test_scores = []
    for sm in sel_model:
        if sm in model_names:
            sel_model_idx = model_names.index(sm)
            pred_test_scores.append(pred_score[sel_model_idx][1])
    okcomp.comp1.draw_roc([np.array(y_test_sel[label])] * len(pred_test_scores), 
                          pred_test_scores, 
                          labels=sel_model, title=f"Model AUC")
    plt.savefig(f'img/model_roc_test.svg', bbox_inches = 'tight')

DCA 决策曲线
[29]
from onekey_algo.custom.components.comp1 import plot_DCA

for pred_score, label in zip(pred_scores, labels):
    pred_test_scores = []
    for sm in sel_model:
        if sm in model_names:
            sel_model_idx = model_names.index(sm)
            okcomp.comp1.plot_DCA(pred_score[sel_model_idx][1][:,1], np.array(y_test_sel[label]),
                                  title=f'Rad Model {sm} DCA')
            plt.savefig(f'img/model_{sm}_dca.svg', bbox_inches = 'tight')

绘制混淆矩阵
绘制混淆矩阵，混淆矩阵解释 sel_model = ['SVM', 'KNN']参数为想要绘制的模型对应的参数。

如果需要修改标签到名称的映射，修改class_mapping={1:'1', 0:'0'}

[30]
# 设置绘制参数
sel_model = model_names
c_matrix = {}

for sm in sel_model:
    if sm in model_names:
        sel_model_idx = model_names.index(sm)
        for idx, label in enumerate(labels):
            cm = okcomp.comp1.calc_confusion_matrix(predictions[idx][sel_model_idx][-1], y_test_sel[label],
#                                                     sel_idx = pred_sel_idx[idx][sel_model_idx],
                                                    class_mapping={1:'1', 0:'0'}, num_classes=2)
            c_matrix[label] = cm
            plt.figure(figsize=(5, 4))
            plt.title(f'Rad Model:{sm}')
            okcomp.comp1.draw_matrix(cm, norm=False, annot=True, cmap='Blues', fmt='.3g')
            plt.savefig(f'img/model_{sm}_cm.svg', bbox_inches = 'tight')

样本预测直方图
绘制每个样本的预测结果以及对应的真实结果, 图例中label=xx可以修改成自己类别的真实标签。

[31]
sel_model = model_names
c_matrix = {}

for sm in sel_model:
    if sm in model_names:
        sel_model_idx = model_names.index(sm)
        for idx, label in enumerate(labels):            
            okcomp.comp1.draw_predict_score(pred_scores[idx][sel_model_idx][-1], y_test_sel[label])
            plt.title(f'{sm} sample predict score')
            plt.legend(labels=["label=0","label=1"],loc="lower right") 
            plt.savefig(f'img/model_{sm}_sample_dis.svg', bbox_inches = 'tight')
            plt.show()

保存模型结果
可以把模型预测的标签结果以及每个类别的概率都保存下来。

[32]
import os
import numpy as np

os.makedirs('results', exist_ok=True)
sel_model = sel_model

for idx, label in enumerate(labels):
    for sm in sel_model:
        if sm in model_names:
            sel_model_idx = model_names.index(sm)
            target = targets[idx][sel_model_idx]
            # 预测训练集和测试集数据。
            train_indexes = np.reshape(np.array(train_ids), (-1, 1)).astype(str)
            test_indexes = np.reshape(np.array(test_ids), (-1, 1)).astype(str)
            y_train_pred_scores = target.predict_proba(X_train_sel)
            y_test_pred_scores = target.predict_proba(X_test_sel)
            columns = ['ID'] + [f"{label}-{i}"for i in range(y_test_pred_scores.shape[1])]
            # 保存预测的训练集和测试集结果
            result_train = pd.DataFrame(np.concatenate([train_indexes, y_train_pred_scores], axis=1), columns=columns)
            result_train.to_csv(f'results/{sm}_Rad_train.csv', index=False)
            result_test = pd.DataFrame(np.concatenate([test_indexes, y_test_pred_scores], axis=1), columns=columns)
            result_test.to_csv(f'results/{sm}_Rad_test.csv', index=False)
