
#config_multi_task.yaml

tasks:
  - name: Blood_stasis
    label_file: F:\graduate\tongue\xinxueguan1397\xueyu\xueyu_labelxuerad.csv
    task_column: label
    models: [Blood_stasis]

  - name: CVD
    label_file: F:\graduate\tongue\xinxueguan1397\xinxueguan\labelxuerad.csv
    task_column: label
    models: [CVD]

  - name: Dampness_syndrome
    label_file: F:\graduate\tongue\xinxueguan1397\shizheng\shizheng_labelxuerad.csv
    task_column: label
    models: [Dampness_syndrome]

  - name: Qi_deficiency
    label_file: F:\graduate\tongue\xinxueguan1397\qixu\qixu_labelxuerad.csv
    task_column: label
    models: [Qi_deficiency]

results_dir: D:\onekey\onekey_comp\comp2-ÁªìÊûÑÂåñÊï∞ÊçÆ\final1\result
output_dir: ./multi_task_output

# Á≠õÈÄâÁöÑÊ®°Âûã
sel_model: SVM


# Ê±áÊÄªÊâÄÊúâÊ®°Âûã‰∫§Âèâ‰ªªÂä°ÁöÑÁªìÊûú

import os
import yaml
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from onekey_algo.custom.components import comp1 as okcomp
from onekey_algo.custom.components.metrics import analysis_pred_binary
from onekey_algo.custom.components.delong import delong_roc_test
from onekey_algo.custom.components.metrics import NRI, IDI
from onekey_algo.custom.components import stats
import matplotlib.gridspec as gridspec

def read_pred(path, model_name):
    """ËØªÂèñÊ®°ÂûãËæìÂá∫ÔºåËøîÂõû ID + Ê≠£Á±ªÊ¶ÇÁéá"""
    df = pd.read_csv(path)
    if 'label-1' in df.columns:
        return df[['ID', 'label-1']].rename(columns={'label-1': f'{model_name}_pred'})
    raise ValueError(f"{path} Áº∫Â∞ë label-1 Âàó")

def main():
    # ËØªÂèñÈÖçÁΩÆ
    with open('config_multi_task.yaml', 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f)

    os.makedirs(cfg['output_dir'], exist_ok=True)
    plt.rcParams['figure.dpi'] = 300
    plt.rcParams['font.size'] = 8
    
    # Â≠òÂÇ®Ê±áÊÄªÊåáÊ†áÂíåÂ§ö‰ªªÂä°ROCÊï∞ÊçÆ
    summary_metrics = []
    task_val_data = {}
    
    # 1. Â§ÑÁêÜÊØè‰∏™‰ªªÂä°
    for task in cfg['tasks']:
        task_name = task['name']
        label_file = task['label_file']
        task_col = task['task_column']
        print(f'üîç Processing task: {task_name}')

        label_df = pd.read_csv(label_file)[['ID', task_col]]
        task_val_data[task_name] = {'y_true': None, 'model_scores': {}}

        for subset in ['train', 'val']:
            print(f'üìä Subset: {subset}')
            all_df = None
            
            # ÂêàÂπ∂Ê®°ÂûãÈ¢ÑÊµãÁªìÊûú
            for mn in cfg['compare_model']:
                path = os.path.join(cfg['results_dir'], f'{mn}_{subset}.csv')
                pred = read_pred(path, mn)
                if all_df is None:
                    all_df = pred
                else:
                    all_df = pd.merge(all_df, pred, on='ID', how='inner')
            
            all_df = pd.merge(all_df, label_df, on='ID', how='inner').dropna()
            y_true = np.array(all_df[task_col])
            y_scores = [np.array(all_df[f'{mn}_pred']) for mn in cfg['compare_model']]
            
            out_dir = os.path.join(cfg['output_dir'], task_name, subset)
            os.makedirs(out_dir, exist_ok=True)
            
            # ‰øùÂ≠òÈ™åËØÅÈõÜÊï∞ÊçÆÁî®‰∫éÊ±áÊÄªÂàÜÊûê
            if subset == 'val':
                task_val_data[task_name]['y_true'] = y_true
                for i, mn in enumerate(cfg['compare_model']):
                    task_val_data[task_name]['model_scores'][mn] = y_scores[i]
            
            # ROCÊõ≤Á∫ø
            fig = plt.figure(figsize=(8, 6))
            okcomp.draw_roc([y_true] * len(cfg['compare_model']), y_scores,
                            labels=cfg['compare_model'], title=f'{task_name} - {subset} AUC')
            plt.savefig(os.path.join(out_dir, 'auc.svg'), bbox_inches='tight')
            plt.close(fig)
            
            # ÊÄßËÉΩÊåáÊ†á
            metrics = []
            youden = {}
            for mn, score in zip(cfg['compare_model'], y_scores):
                acc, auc, ci, tpr, tnr, ppv, npv, prec, rec, f1, th = analysis_pred_binary(y_true, score)
                ci_str = f'{ci[0]:.4f} - {ci[1]:.4f}'
                metrics.append((mn, acc, auc, ci_str, tpr, tnr, ppv, npv, prec, rec, f1, th, subset))
                youden[mn] = th
                
                # Êî∂ÈõÜÈ™åËØÅÈõÜÊåáÊ†áÁî®‰∫éÊ±áÊÄª
                if subset == 'val':
                    summary_metrics.append({
                        'Model': mn,
                        'Task': task_name,
                        'AUC': auc,
                        'Accuracy': acc,
                        'Sensitivity': tpr,
                        'Specificity': tnr,
                        'F1': f1
                    })
            
            metrics_df = pd.DataFrame(metrics, columns=[
                'Signature', 'Accuracy', 'AUC', '95% CI', 'Sensitivity', 'Specificity',
                'PPV', 'NPV', 'Precision', 'Recall', 'F1', 'Threshold', 'Cohort'
            ])
            metrics_df.to_csv(os.path.join(out_dir, 'metrics.csv'), index=False)
            
            # DelongÊ£ÄÈ™å
            cm = np.full((len(cfg['compare_model']), len(cfg['compare_model'])), np.nan)
            for i, mni in enumerate(cfg['compare_model']):
                for j, mnj in enumerate(cfg['compare_model']):
                    if i > j:
                        cm[i, j] = delong_roc_test(y_true, all_df[f'{mni}_pred'], all_df[f'{mnj}_pred'])[0][0]
            cm_df = pd.DataFrame(cm[1:, :-1], index=cfg['compare_model'][1:], columns=cfg['compare_model'][:-1])
            
            fig = plt.figure(figsize=(8, 6))
            okcomp.draw_matrix(cm_df, annot=True, cmap='jet_r', cbar=True)
            plt.title(f'{task_name} - {subset} Delong')
            plt.savefig(os.path.join(out_dir, 'delong.svg'), bbox_inches='tight')
            plt.close(fig)
            
            # NRI
            cm = np.zeros((len(cfg['compare_model']), len(cfg['compare_model'])))
            for i, mni in enumerate(cfg['compare_model']):
                for j, mnj in enumerate(cfg['compare_model']):
                    cm[i, j] = NRI(all_df[f'{mni}_pred'] > youden[mni],
                                   all_df[f'{mnj}_pred'] > youden[mnj], y_true)
            cm_df = pd.DataFrame(cm, index=cfg['compare_model'], columns=cfg['compare_model'])
            
            fig = plt.figure(figsize=(8, 6))
            okcomp.draw_matrix(cm_df, annot=True, cmap='jet_r', cbar=True)
            plt.title(f'{task_name} - {subset} NRI')
            plt.savefig(os.path.join(out_dir, 'nri.svg'), bbox_inches='tight')
            plt.close(fig)
            
            # IDI
            idi = np.zeros((len(cfg['compare_model']), len(cfg['compare_model'])))
            idi_p = np.zeros_like(idi)
            for i, mni in enumerate(cfg['compare_model']):
                for j, mnj in enumerate(cfg['compare_model']):
                    idi[i, j], idi_p[i, j] = IDI(all_df[f'{mni}_pred'], all_df[f'{mnj}_pred'], y_true, with_p=True)
            
            for arr, name in zip([idi, idi_p], ['IDI', 'IDI_pvalue']):
                df = pd.DataFrame(arr, index=cfg['compare_model'], columns=cfg['compare_model'])
                
                fig = plt.figure(figsize=(8, 6))
                okcomp.draw_matrix(df, annot=True, cmap='jet_r', cbar=True)
                plt.title(f'{task_name} - {subset} {name}')
                plt.savefig(os.path.join(out_dir, f'{name.lower()}.svg'), bbox_inches='tight')
                plt.close(fig)
            
            # DCA
            fig = plt.figure(figsize=(8, 6))
            okcomp.plot_DCA([all_df[f'{mn}_pred'] for mn in cfg['compare_model']], y_true,
                            title=f'{task_name} - {subset} DCA', labels=cfg['compare_model'], y_min=-0.15)
            plt.savefig(os.path.join(out_dir, 'dca.svg'), bbox_inches='tight')
            plt.close(fig)
            
            # Calibration
            fig = plt.figure(figsize=(8, 6))
            okcomp.draw_calibration(pred_scores=y_scores, n_bins=5, y_test=y_true, model_names=cfg['compare_model'])
            plt.savefig(os.path.join(out_dir, 'calibration.svg'), bbox_inches='tight')
            plt.close(fig)
            
            # Hosmer-Lemeshow
            hosmer = []
            for mn in cfg['compare_model']:
                result = stats.hosmer_lemeshow_test(y_true, all_df[f'{mn}_pred'], bins=15)
                if isinstance(result, tuple) and len(result) == 2:
                    stat, p = result
                else:
                    stat, p = np.nan, result
                hosmer.append((mn, stat, p))
            
            hosmer_df = pd.DataFrame(hosmer, columns=['Model', 'H-L Stat', 'p-value'])
            hosmer_df.to_csv(os.path.join(out_dir, 'hosmer_lemeshow.csv'), index=False)
    
    # 2. Â§ö‰ªªÂä°ROCÂèØËßÜÂåñ
    if task_val_data:
        print("üìà Generating multi-task visualizations...")
        summary_dir = os.path.join(cfg['output_dir'], 'summary')
        os.makedirs(summary_dir, exist_ok=True)
        
        # Â§ö‰ªªÂä°ROCÊõ≤Á∫ø
        fig = plt.figure(figsize=(12, 10))
        gs = gridspec.GridSpec(2, 2)
        axs = [plt.subplot(gs[0, 0]), plt.subplot(gs[0, 1]), 
               plt.subplot(gs[1, 0]), plt.subplot(gs[1, 1])]
        
        for idx, task_name in enumerate(task_val_data.keys()):
            data = task_val_data[task_name]
            y_true = data['y_true']
            
            for mn in cfg['compare_model']:
                if mn in data['model_scores']:
                    score = data['model_scores'][mn]
                    fpr, tpr, _ = okcomp.roc_curve(y_true, score)
                    roc_auc = okcomp.auc(fpr, tpr)
                    axs[idx].plot(fpr, tpr, label=f'{mn} (AUC={roc_auc:.3f})')
            
            axs[idx].plot([0, 1], [0, 1], 'k--')
            axs[idx].set_xlim([0.0, 1.0])
            axs[idx].set_ylim([0.0, 1.05])
            axs[idx].set_xlabel('False Positive Rate')
            axs[idx].set_ylabel('True Positive Rate')
            axs[idx].set_title(f'Task: {task_name}')
            axs[idx].legend(loc="lower right", fontsize=8)
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig(os.path.join(summary_dir, 'multi_task_roc.svg'))
        plt.close(fig)
        
        # ‰øùÂ≠òÊ±áÊÄªÊåáÊ†á
        if summary_metrics:
            summary_df = pd.DataFrame(summary_metrics)
            summary_df.to_csv(os.path.join(summary_dir, 'multi_task_metrics.csv'), index=False)
            print(f"‚úÖ Saved summary metrics to {os.path.join(summary_dir, 'multi_task_metrics.csv')}")
    
    print('‚úÖ ÂÖ®ÈÉ®ÂÆåÊàêÔºÅ')

if __name__ == "__main__":
    main()

# Ê±áÊÄªÂçï‰ªªÂä°Ê®°ÂûãÁöÑÁªìÊûú

import os
import yaml
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import gridspec
from onekey_algo.custom.components import comp1 as okcomp
from onekey_algo.custom.components.metrics import analysis_pred_binary
from onekey_algo.custom.components import stats
import sys

def read_pred(path, model_name):
    """ËØªÂèñÊ®°ÂûãËæìÂá∫ÔºåËøîÂõû ID + Ê≠£Á±ªÊ¶ÇÁéá"""
    df = pd.read_csv(path)
    if 'label-1' in df.columns:
        return df[['ID', 'label-1']].rename(columns={'label-1': f'{model_name}_pred'})
    raise ValueError(f"{path} Áº∫Â∞ë label-1 Âàó")

def main():
    # ËØªÂèñÈÖçÁΩÆ
    with open('config_multi_task.yaml', 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f)

    os.makedirs(cfg['output_dir'], exist_ok=True)
    plt.rcParams['figure.dpi'] = 300
    plt.rcParams['font.size'] = 9
    
    # È™åËØÅÊ®°ÂûãÊï∞ÈáèÂíå‰ªªÂä°Êï∞ÈáèÊòØÂê¶ÂåπÈÖç
    if len(cfg['tasks']) != len(cfg['compare_model']):
        print(f"‚ö†Ô∏è Ë≠¶Âëä: ‰ªªÂä°Êï∞Èáè({len(cfg['tasks'])})ÂíåÊ®°ÂûãÊï∞Èáè({len(cfg['compare_model'])})‰∏çÂåπÈÖçÔºÅ")
        print("Â∞Ü‰ΩøÁî®ÊúÄÂ∞èÊï∞ÈáèËøõË°åÂåπÈÖç")
        num_pairs = min(len(cfg['tasks']), len(cfg['compare_model']))
    else:
        num_pairs = len(cfg['tasks'])
    
    # Â≠òÂÇ®Ê±áÊÄªÊåáÊ†á
    summary_metrics = []
    roc_data = []  # Â≠òÂÇ®ROCÊï∞ÊçÆÁî®‰∫éÂ§ö‰ªªÂä°Âõæ
    
    # 1. Â§ÑÁêÜÊØè‰∏™‰ªªÂä°ÂèäÂÖ∂ÂØπÂ∫îÁöÑÊ®°Âûã
    for i in range(num_pairs):
        task = cfg['tasks'][i]
        model_name = cfg['compare_model'][i]
        
        task_name = task['name']
        label_file = task['label_file']
        task_col = task['task_column']
        print(f'\nüîç Â§ÑÁêÜ‰ªªÂä°: {task_name} - Ê®°Âûã: {model_name}')
        
        # ËØªÂèñ‰ªªÂä°Ê†áÁ≠æ
        label_df = pd.read_csv(label_file)[['ID', task_col]]
        
        task_metrics = []  # Â≠òÂÇ®ËØ•‰ªªÂä°ÁöÑÊâÄÊúâÊåáÊ†á
        
        for subset in ['train', 'val']:
            print(f'  üìä Â≠êÈõÜ: {subset}')
            
            # ËØªÂèñÊ®°ÂûãÈ¢ÑÊµã
            path = os.path.join(cfg['results_dir'], f'{model_name}_{subset}.csv')
            try:
                pred = read_pred(path, model_name)
            except Exception as e:
                print(f"‚ùå ËØªÂèñÈ¢ÑÊµãÊñá‰ª∂Âá∫Èîô: {e}")
                continue
            
            # ÂêàÂπ∂Ê†áÁ≠æ
            all_df = pd.merge(pred, label_df, on='ID', how='inner').dropna()
            
            if all_df.empty:
                print(f"‚ö†Ô∏è Ë≠¶Âëä: {model_name}Âú®{task_name}‰ªªÂä°{subset}Â≠êÈõÜ‰∏äÊ≤°ÊúâÂåπÈÖçÁöÑÊï∞ÊçÆ")
                continue
            
            y_true = np.array(all_df[task_col])
            y_score = np.array(all_df[f'{model_name}_pred'])
            
            # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
            out_dir = os.path.join(cfg['output_dir'], task_name, model_name, subset)
            os.makedirs(out_dir, exist_ok=True)
            
            # ËÆ°ÁÆóÊÄßËÉΩÊåáÊ†á
            print(f"     ‚è≥ ËÆ°ÁÆóÊÄßËÉΩÊåáÊ†á...")
            try:
                acc, auc, ci, tpr, tnr, ppv, npv, prec, rec, f1, th = analysis_pred_binary(y_true, y_score)
                ci_str = f'{ci[0]:.4f} - {ci[1]:.4f}'
                
                # Â≠òÂÇ®ÊåáÊ†á
                metrics_row = {
                    'Model': model_name,
                    'Task': task_name,
                    'Subset': subset,
                    'Accuracy': f'{acc:.4f}',
                    'AUC': f'{auc:.4f}',
                    '95% CI': ci_str,
                    'Sensitivity': f'{tpr:.4f}',
                    'Specificity': f'{tnr:.4f}',
                    'PPV': f'{ppv:.4f}',
                    'NPV': f'{npv:.4f}',
                    'Precision': f'{prec:.4f}',
                    'Recall': f'{rec:.4f}',
                    'F1': f'{f1:.4f}',
                    'Threshold': f'{th:.4f}',
                    'Samples': len(y_true)
                }
                
                task_metrics.append(metrics_row)
                summary_metrics.append(metrics_row)
                
                # ‰øùÂ≠òÊåáÊ†áÂà∞CSV
                pd.DataFrame([metrics_row]).to_csv(os.path.join(out_dir, 'metrics.csv'), index=False)
            except Exception as e:
                print(f"‚ùå ËÆ°ÁÆóÊåáÊ†áÂá∫Èîô: {e}")
                continue
            
            # ÁªòÂà∂ROCÊõ≤Á∫ø
            print(f"     ‚è≥ ÁªòÂà∂ROCÊõ≤Á∫ø...")
            try:
                fig, ax = plt.subplots(figsize=(8, 6))
                okcomp.draw_roc([y_true], [y_score], labels=[model_name], 
                               title=f'{task_name} - {model_name} - {subset} AUC', ax=ax)
                plt.savefig(os.path.join(out_dir, 'auc.svg'), bbox_inches='tight')
                plt.close(fig)
            except Exception as e:
                print(f"‚ùå ÁªòÂà∂ROCÂá∫Èîô: {e}")
            
            # ÁªòÂà∂DCAÊõ≤Á∫ø
            print(f"     ÁªòÂà∂DCAÊõ≤Á∫ø...")
            try:
                fig, ax = plt.subplots(figsize=(8, 6))
                okcomp.plot_DCA([y_score], y_true, title=f'{task_name} - {model_name} - {subset} DCA', 
                               labels=[model_name], y_min=-0.15, ax=ax)
                plt.savefig(os.path.join(out_dir, 'dca.svg'), bbox_inches='tight')
                plt.close(fig)
            except Exception as e:
                print(f"‚ùå ÁªòÂà∂DCAÂá∫Èîô: {e}")
            
            # ÁªòÂà∂Ê†°ÂáÜÊõ≤Á∫ø
            print(f"     ‚è≥ ÁªòÂà∂Ê†°ÂáÜÊõ≤Á∫ø...")
            try:
                fig, ax = plt.subplots(figsize=(8, 6))
                okcomp.draw_calibration(pred_scores=[y_score], n_bins=5, y_test=y_true, 
                                       model_names=[model_name], ax=ax)
                plt.savefig(os.path.join(out_dir, 'calibration.svg'), bbox_inches='tight')
                plt.close(fig)
            except Exception as e:
                print(f"‚ùå ÁªòÂà∂Ê†°ÂáÜÊõ≤Á∫øÂá∫Èîô: {e}")
            
            # Hosmer-LemeshowÊ£ÄÈ™å
            print(f"     ËÆ°ÁÆóHosmer-LemeshowÊ£ÄÈ™å...")
            try:
                result = stats.hosmer_lemeshow_test(y_true, y_score, bins=15)
                if isinstance(result, tuple) and len(result) == 2:
                    stat, p = result
                else:
                    stat, p = np.nan, result
                
                hl_row = {
                    'Model': model_name,
                    'Task': task_name,
                    'Subset': subset,
                    'H-L Stat': f'{stat:.4f}' if not np.isnan(stat) else 'N/A',
                    'p-value': f'{p:.4f}'
                }
                pd.DataFrame([hl_row]).to_csv(os.path.join(out_dir, 'hosmer_lemeshow.csv'), index=False)
            except Exception as e:
                print(f"‚ùå ËÆ°ÁÆóHosmer-LemeshowÊ£ÄÈ™åÂá∫Èîô: {e}")
            
            # Â≠òÂÇ®ROCÊï∞ÊçÆÁî®‰∫éÂ§ö‰ªªÂä°Âõæ
            try:
                fpr, tpr, _ = okcomp.roc_curve(y_true, y_score)
                roc_auc = okcomp.auc(fpr, tpr)
                roc_data.append({
                    'task': task_name,
                    'model': model_name,
                    'subset': subset,
                    'fpr': fpr,
                    'tpr': tpr,
                    'auc': roc_auc
                })
            except:
                pass
            
            print(f"    ‚úÖ {subset}Â≠êÈõÜÂ§ÑÁêÜÂÆåÊàê")
        
        # ‰øùÂ≠òËØ•‰ªªÂä°ÁöÑÊâÄÊúâÊåáÊ†á
        if task_metrics:
            task_metrics_df = pd.DataFrame(task_metrics)
            task_dir = os.path.join(cfg['output_dir'], task_name, model_name)
            task_metrics_df.to_csv(os.path.join(task_dir, 'all_metrics.csv'), index=False)
    
    # 2. ÁîüÊàêÂ§ö‰ªªÂä°ROCÂØπÊØîÂõæ
    if roc_data:
        print("\nüìà ÁîüÊàêÂ§ö‰ªªÂä°ROCÂõæ...")
        summary_dir = os.path.join(cfg['output_dir'], 'summary')
        os.makedirs(summary_dir, exist_ok=True)
        
        # ÂàõÂª∫Â§ö‰ªªÂä°ROCÂõæ
        fig = plt.figure(figsize=(14, 10))
        gs = gridspec.GridSpec(2, 2)
        axes = [fig.add_subplot(gs[i//2, i%2]) for i in range(4)]
        
        # Êî∂ÈõÜ‰ªªÂä°Êï∞ÊçÆ
        task_plots = {}
        for data in roc_data:
            task = data['task']
            if task not in task_plots:
                task_plots[task] = []
            task_plots[task].append(data)
        
        # ÁªòÂà∂ÊØè‰∏™‰ªªÂä°ÁöÑROCÊõ≤Á∫ø
        for i, (task_name, task_data) in enumerate(task_plots.items()):
            if i >= 4:  # ÊúÄÂ§öÁªòÂà∂4‰∏™‰ªªÂä°
                break
                
            ax = axes[i]
            for data in task_data:
                label = f"{data['model']} ({data['subset']}, AUC={data['auc']:.3f})"
                ax.plot(data['fpr'], data['tpr'], label=label)
            
            ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(f'Task: {task_name}', fontsize=12)
            ax.legend(loc="lower right", fontsize=8)
            ax.grid(True, linestyle='--', alpha=0.7)
        
        plt.suptitle('Multi-Task ROC Curves', fontsize=16, y=0.98)
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.savefig(os.path.join(summary_dir, 'multi_task_roc.svg'), bbox_inches='tight')
        plt.close(fig)
        print(f"‚úÖ Â§ö‰ªªÂä°ROCÂõæÂ∑≤‰øùÂ≠òËá≥: {os.path.join(summary_dir, 'multi_task_roc.svg')}")
    
    # 3. ‰øùÂ≠òÊ±áÊÄªÊåáÊ†á
    if summary_metrics:
        summary_dir = os.path.join(cfg['output_dir'], 'summary')
        os.makedirs(summary_dir, exist_ok=True)
        
        summary_df = pd.DataFrame(summary_metrics)
        summary_file = os.path.join(summary_dir, 'multi_task_metrics.csv')
        summary_df.to_csv(summary_file, index=False)
        print(f"‚úÖ Ê±áÊÄªÊåáÊ†áÂ∑≤‰øùÂ≠òËá≥: {summary_file}")
    
    print('\nüéâ ÂÖ®ÈÉ®ÂÆåÊàêÔºÅ')

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"‚ùå Á®ãÂ∫èËøêË°åÂá∫Èîô: {e}")
        sys.exit(1)
