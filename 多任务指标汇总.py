
#config_multi_task.yaml

tasks:
  - name: Blood_stasis
    label_file: F:\graduate\tongue\xinxueguan1397\xueyu\xueyu_labelxuerad.csv
    task_column: label
    models: [Blood_stasis]

  - name: CVD
    label_file: F:\graduate\tongue\xinxueguan1397\xinxueguan\labelxuerad.csv
    task_column: label
    models: [CVD]

  - name: Dampness_syndrome
    label_file: F:\graduate\tongue\xinxueguan1397\shizheng\shizheng_labelxuerad.csv
    task_column: label
    models: [Dampness_syndrome]

  - name: Qi_deficiency
    label_file: F:\graduate\tongue\xinxueguan1397\qixu\qixu_labelxuerad.csv
    task_column: label
    models: [Qi_deficiency]

results_dir: D:\onekey\onekey_comp\comp2-ç»“æ„åŒ–æ•°æ®\final1\result
output_dir: ./multi_task_output

# ç­›é€‰çš„æ¨¡å‹
sel_model: SVM


# æ±‡æ€»æ‰€æœ‰æ¨¡å‹äº¤å‰ä»»åŠ¡çš„ç»“æœ

import os
import yaml
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from onekey_algo.custom.components import comp1 as okcomp
from onekey_algo.custom.components.metrics import analysis_pred_binary
from onekey_algo.custom.components.delong import delong_roc_test
from onekey_algo.custom.components.metrics import NRI, IDI
from onekey_algo.custom.components import stats
import matplotlib.gridspec as gridspec

def read_pred(path, model_name):
    """è¯»å–æ¨¡å‹è¾“å‡ºï¼Œè¿”å› ID + æ­£ç±»æ¦‚ç‡"""
    df = pd.read_csv(path)
    if 'label-1' in df.columns:
        return df[['ID', 'label-1']].rename(columns={'label-1': f'{model_name}_pred'})
    raise ValueError(f"{path} ç¼ºå°‘ label-1 åˆ—")

def main():
    # è¯»å–é…ç½®
    with open('config_multi_task.yaml', 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f)

    os.makedirs(cfg['output_dir'], exist_ok=True)
    plt.rcParams['figure.dpi'] = 300
    plt.rcParams['font.size'] = 8
    
    # å­˜å‚¨æ±‡æ€»æŒ‡æ ‡å’Œå¤šä»»åŠ¡ROCæ•°æ®
    summary_metrics = []
    task_val_data = {}
    
    # 1. å¤„ç†æ¯ä¸ªä»»åŠ¡
    for task in cfg['tasks']:
        task_name = task['name']
        label_file = task['label_file']
        task_col = task['task_column']
        print(f'ğŸ” Processing task: {task_name}')

        label_df = pd.read_csv(label_file)[['ID', task_col]]
        task_val_data[task_name] = {'y_true': None, 'model_scores': {}}

        for subset in ['train', 'val']:
            print(f'ğŸ“Š Subset: {subset}')
            all_df = None
            
            # åˆå¹¶æ¨¡å‹é¢„æµ‹ç»“æœ
            for mn in cfg['compare_model']:
                path = os.path.join(cfg['results_dir'], f'{mn}_{subset}.csv')
                pred = read_pred(path, mn)
                if all_df is None:
                    all_df = pred
                else:
                    all_df = pd.merge(all_df, pred, on='ID', how='inner')
            
            all_df = pd.merge(all_df, label_df, on='ID', how='inner').dropna()
            y_true = np.array(all_df[task_col])
            y_scores = [np.array(all_df[f'{mn}_pred']) for mn in cfg['compare_model']]
            
            out_dir = os.path.join(cfg['output_dir'], task_name, subset)
            os.makedirs(out_dir, exist_ok=True)
            
            # ä¿å­˜éªŒè¯é›†æ•°æ®ç”¨äºæ±‡æ€»åˆ†æ
            if subset == 'val':
                task_val_data[task_name]['y_true'] = y_true
                for i, mn in enumerate(cfg['compare_model']):
                    task_val_data[task_name]['model_scores'][mn] = y_scores[i]
            
            # ROCæ›²çº¿
            fig = plt.figure(figsize=(8, 6))
            okcomp.draw_roc([y_true] * len(cfg['compare_model']), y_scores,
                            labels=cfg['compare_model'], title=f'{task_name} - {subset} AUC')
            plt.savefig(os.path.join(out_dir, 'auc.svg'), bbox_inches='tight')
            plt.close(fig)
            
            # æ€§èƒ½æŒ‡æ ‡
            metrics = []
            youden = {}
            for mn, score in zip(cfg['compare_model'], y_scores):
                acc, auc, ci, tpr, tnr, ppv, npv, prec, rec, f1, th = analysis_pred_binary(y_true, score)
                ci_str = f'{ci[0]:.4f} - {ci[1]:.4f}'
                metrics.append((mn, acc, auc, ci_str, tpr, tnr, ppv, npv, prec, rec, f1, th, subset))
                youden[mn] = th
                
                # æ”¶é›†éªŒè¯é›†æŒ‡æ ‡ç”¨äºæ±‡æ€»
                if subset == 'val':
                    summary_metrics.append({
                        'Model': mn,
                        'Task': task_name,
                        'AUC': auc,
                        'Accuracy': acc,
                        'Sensitivity': tpr,
                        'Specificity': tnr,
                        'F1': f1
                    })
            
            metrics_df = pd.DataFrame(metrics, columns=[
                'Signature', 'Accuracy', 'AUC', '95% CI', 'Sensitivity', 'Specificity',
                'PPV', 'NPV', 'Precision', 'Recall', 'F1', 'Threshold', 'Cohort'
            ])
            metrics_df.to_csv(os.path.join(out_dir, 'metrics.csv'), index=False)
            
            # Delongæ£€éªŒ
            cm = np.full((len(cfg['compare_model']), len(cfg['compare_model'])), np.nan)
            for i, mni in enumerate(cfg['compare_model']):
                for j, mnj in enumerate(cfg['compare_model']):
                    if i > j:
                        cm[i, j] = delong_roc_test(y_true, all_df[f'{mni}_pred'], all_df[f'{mnj}_pred'])[0][0]
            cm_df = pd.DataFrame(cm[1:, :-1], index=cfg['compare_model'][1:], columns=cfg['compare_model'][:-1])
            
            fig = plt.figure(figsize=(8, 6))
            okcomp.draw_matrix(cm_df, annot=True, cmap='jet_r', cbar=True)
            plt.title(f'{task_name} - {subset} Delong')
            plt.savefig(os.path.join(out_dir, 'delong.svg'), bbox_inches='tight')
            plt.close(fig)
            
            # NRI
            cm = np.zeros((len(cfg['compare_model']), len(cfg['compare_model'])))
            for i, mni in enumerate(cfg['compare_model']):
                for j, mnj in enumerate(cfg['compare_model']):
                    cm[i, j] = NRI(all_df[f'{mni}_pred'] > youden[mni],
                                   all_df[f'{mnj}_pred'] > youden[mnj], y_true)
            cm_df = pd.DataFrame(cm, index=cfg['compare_model'], columns=cfg['compare_model'])
            
            fig = plt.figure(figsize=(8, 6))
            okcomp.draw_matrix(cm_df, annot=True, cmap='jet_r', cbar=True)
            plt.title(f'{task_name} - {subset} NRI')
            plt.savefig(os.path.join(out_dir, 'nri.svg'), bbox_inches='tight')
            plt.close(fig)
            
            # IDI
            idi = np.zeros((len(cfg['compare_model']), len(cfg['compare_model'])))
            idi_p = np.zeros_like(idi)
            for i, mni in enumerate(cfg['compare_model']):
                for j, mnj in enumerate(cfg['compare_model']):
                    idi[i, j], idi_p[i, j] = IDI(all_df[f'{mni}_pred'], all_df[f'{mnj}_pred'], y_true, with_p=True)
            
            for arr, name in zip([idi, idi_p], ['IDI', 'IDI_pvalue']):
                df = pd.DataFrame(arr, index=cfg['compare_model'], columns=cfg['compare_model'])
                
                fig = plt.figure(figsize=(8, 6))
                okcomp.draw_matrix(df, annot=True, cmap='jet_r', cbar=True)
                plt.title(f'{task_name} - {subset} {name}')
                plt.savefig(os.path.join(out_dir, f'{name.lower()}.svg'), bbox_inches='tight')
                plt.close(fig)
            
            # DCA
            fig = plt.figure(figsize=(8, 6))
            okcomp.plot_DCA([all_df[f'{mn}_pred'] for mn in cfg['compare_model']], y_true,
                            title=f'{task_name} - {subset} DCA', labels=cfg['compare_model'], y_min=-0.15)
            plt.savefig(os.path.join(out_dir, 'dca.svg'), bbox_inches='tight')
            plt.close(fig)
            
            # Calibration
            fig = plt.figure(figsize=(8, 6))
            okcomp.draw_calibration(pred_scores=y_scores, n_bins=5, y_test=y_true, model_names=cfg['compare_model'])
            plt.savefig(os.path.join(out_dir, 'calibration.svg'), bbox_inches='tight')
            plt.close(fig)
            
            # Hosmer-Lemeshow
            hosmer = []
            for mn in cfg['compare_model']:
                result = stats.hosmer_lemeshow_test(y_true, all_df[f'{mn}_pred'], bins=15)
                if isinstance(result, tuple) and len(result) == 2:
                    stat, p = result
                else:
                    stat, p = np.nan, result
                hosmer.append((mn, stat, p))
            
            hosmer_df = pd.DataFrame(hosmer, columns=['Model', 'H-L Stat', 'p-value'])
            hosmer_df.to_csv(os.path.join(out_dir, 'hosmer_lemeshow.csv'), index=False)
    
    # 2. å¤šä»»åŠ¡ROCå¯è§†åŒ–
    if task_val_data:
        print("ğŸ“ˆ Generating multi-task visualizations...")
        summary_dir = os.path.join(cfg['output_dir'], 'summary')
        os.makedirs(summary_dir, exist_ok=True)
        
        # å¤šä»»åŠ¡ROCæ›²çº¿
        fig = plt.figure(figsize=(12, 10))
        gs = gridspec.GridSpec(2, 2)
        axs = [plt.subplot(gs[0, 0]), plt.subplot(gs[0, 1]), 
               plt.subplot(gs[1, 0]), plt.subplot(gs[1, 1])]
        
        for idx, task_name in enumerate(task_val_data.keys()):
            data = task_val_data[task_name]
            y_true = data['y_true']
            
            for mn in cfg['compare_model']:
                if mn in data['model_scores']:
                    score = data['model_scores'][mn]
                    fpr, tpr, _ = okcomp.roc_curve(y_true, score)
                    roc_auc = okcomp.auc(fpr, tpr)
                    axs[idx].plot(fpr, tpr, label=f'{mn} (AUC={roc_auc:.3f})')
            
            axs[idx].plot([0, 1], [0, 1], 'k--')
            axs[idx].set_xlim([0.0, 1.0])
            axs[idx].set_ylim([0.0, 1.05])
            axs[idx].set_xlabel('False Positive Rate')
            axs[idx].set_ylabel('True Positive Rate')
            axs[idx].set_title(f'Task: {task_name}')
            axs[idx].legend(loc="lower right", fontsize=8)
        
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig(os.path.join(summary_dir, 'multi_task_roc.svg'))
        plt.close(fig)
        
        # ä¿å­˜æ±‡æ€»æŒ‡æ ‡
        if summary_metrics:
            summary_df = pd.DataFrame(summary_metrics)
            summary_df.to_csv(os.path.join(summary_dir, 'multi_task_metrics.csv'), index=False)
            print(f"âœ… Saved summary metrics to {os.path.join(summary_dir, 'multi_task_metrics.csv')}")
    
    print('âœ… å…¨éƒ¨å®Œæˆï¼')

if __name__ == "__main__":
    main()

# æ±‡æ€»å•ä»»åŠ¡æ¨¡å‹çš„ç»“æœ

import os
import yaml
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import gridspec
from onekey_algo.custom.components import comp1 as okcomp
from onekey_algo.custom.components.metrics import analysis_pred_binary
from onekey_algo.custom.components import stats
import sys

def read_pred(path, model_name):
    """è¯»å–æ¨¡å‹è¾“å‡ºï¼Œè¿”å› ID + æ­£ç±»æ¦‚ç‡"""
    df = pd.read_csv(path)
    if 'label-1' in df.columns:
        return df[['ID', 'label-1']].rename(columns={'label-1': f'{model_name}_pred'})
    raise ValueError(f"{path} ç¼ºå°‘ label-1 åˆ—")

def main():
    # è¯»å–é…ç½®
    with open('config_multi_task.yaml', 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f)

    os.makedirs(cfg['output_dir'], exist_ok=True)
    plt.rcParams['figure.dpi'] = 300
    plt.rcParams['font.size'] = 9
    
    # éªŒè¯æ¨¡å‹æ•°é‡å’Œä»»åŠ¡æ•°é‡æ˜¯å¦åŒ¹é…
    if len(cfg['tasks']) != len(cfg['compare_model']):
        print(f"âš ï¸ è­¦å‘Š: ä»»åŠ¡æ•°é‡({len(cfg['tasks'])})å’Œæ¨¡å‹æ•°é‡({len(cfg['compare_model'])})ä¸åŒ¹é…ï¼")
        print("å°†ä½¿ç”¨æœ€å°æ•°é‡è¿›è¡ŒåŒ¹é…")
        num_pairs = min(len(cfg['tasks']), len(cfg['compare_model']))
    else:
        num_pairs = len(cfg['tasks'])
    
    # å­˜å‚¨æ±‡æ€»æŒ‡æ ‡
    summary_metrics = []
    roc_data = []  # å­˜å‚¨ROCæ•°æ®ç”¨äºå¤šä»»åŠ¡å›¾
    
    # 1. å¤„ç†æ¯ä¸ªä»»åŠ¡åŠå…¶å¯¹åº”çš„æ¨¡å‹
    for i in range(num_pairs):
        task = cfg['tasks'][i]
        model_name = cfg['compare_model'][i]
        
        task_name = task['name']
        label_file = task['label_file']
        task_col = task['task_column']
        print(f'\nğŸ” å¤„ç†ä»»åŠ¡: {task_name} - æ¨¡å‹: {model_name}')
        
        # è¯»å–ä»»åŠ¡æ ‡ç­¾
        label_df = pd.read_csv(label_file)[['ID', task_col]]
        
        task_metrics = []  # å­˜å‚¨è¯¥ä»»åŠ¡çš„æ‰€æœ‰æŒ‡æ ‡
        
        for subset in ['train', 'val']:
            print(f'  ğŸ“Š å­é›†: {subset}')
            
            # è¯»å–æ¨¡å‹é¢„æµ‹
            path = os.path.join(cfg['results_dir'], f'{model_name}_{subset}.csv')
            try:
                pred = read_pred(path, model_name)
            except Exception as e:
                print(f"âŒ è¯»å–é¢„æµ‹æ–‡ä»¶å‡ºé”™: {e}")
                continue
            
            # åˆå¹¶æ ‡ç­¾
            all_df = pd.merge(pred, label_df, on='ID', how='inner').dropna()
            
            if all_df.empty:
                print(f"âš ï¸ è­¦å‘Š: {model_name}åœ¨{task_name}ä»»åŠ¡{subset}å­é›†ä¸Šæ²¡æœ‰åŒ¹é…çš„æ•°æ®")
                continue
            
            y_true = np.array(all_df[task_col])
            y_score = np.array(all_df[f'{model_name}_pred'])
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            out_dir = os.path.join(cfg['output_dir'], task_name, model_name, subset)
            os.makedirs(out_dir, exist_ok=True)
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            print(f"     â³ è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
            try:
                acc, auc, ci, tpr, tnr, ppv, npv, prec, rec, f1, th = analysis_pred_binary(y_true, y_score)
                ci_str = f'{ci[0]:.4f} - {ci[1]:.4f}'
                
                # å­˜å‚¨æŒ‡æ ‡
                metrics_row = {
                    'Model': model_name,
                    'Task': task_name,
                    'Subset': subset,
                    'Accuracy': f'{acc:.4f}',
                    'AUC': f'{auc:.4f}',
                    '95% CI': ci_str,
                    'Sensitivity': f'{tpr:.4f}',
                    'Specificity': f'{tnr:.4f}',
                    'PPV': f'{ppv:.4f}',
                    'NPV': f'{npv:.4f}',
                    'Precision': f'{prec:.4f}',
                    'Recall': f'{rec:.4f}',
                    'F1': f'{f1:.4f}',
                    'Threshold': f'{th:.4f}',
                    'Samples': len(y_true)
                }
                
                task_metrics.append(metrics_row)
                summary_metrics.append(metrics_row)
                
                # ä¿å­˜æŒ‡æ ‡åˆ°CSV
                pd.DataFrame([metrics_row]).to_csv(os.path.join(out_dir, 'metrics.csv'), index=False)
            except Exception as e:
                print(f"âŒ è®¡ç®—æŒ‡æ ‡å‡ºé”™: {e}")
                continue
            
            # ç»˜åˆ¶ROCæ›²çº¿
            print(f"     â³ ç»˜åˆ¶ROCæ›²çº¿...")
            try:
                fig, ax = plt.subplots(figsize=(8, 6))
                okcomp.draw_roc([y_true], [y_score], labels=[model_name], 
                               title=f'{task_name} - {model_name} - {subset} AUC', ax=ax)
                plt.savefig(os.path.join(out_dir, 'auc.svg'), bbox_inches='tight')
                plt.close(fig)
            except Exception as e:
                print(f"âŒ ç»˜åˆ¶ROCå‡ºé”™: {e}")
            
            # ç»˜åˆ¶DCAæ›²çº¿
            print(f"     ç»˜åˆ¶DCAæ›²çº¿...")
            try:
                fig, ax = plt.subplots(figsize=(8, 6))
                okcomp.plot_DCA([y_score], y_true, title=f'{task_name} - {model_name} - {subset} DCA', 
                               labels=[model_name], y_min=-0.15, ax=ax)
                plt.savefig(os.path.join(out_dir, 'dca.svg'), bbox_inches='tight')
                plt.close(fig)
            except Exception as e:
                print(f"âŒ ç»˜åˆ¶DCAå‡ºé”™: {e}")
            
            # ç»˜åˆ¶æ ¡å‡†æ›²çº¿
            print(f"     â³ ç»˜åˆ¶æ ¡å‡†æ›²çº¿...")
            try:
                fig, ax = plt.subplots(figsize=(8, 6))
                okcomp.draw_calibration(pred_scores=[y_score], n_bins=5, y_test=y_true, 
                                       model_names=[model_name], ax=ax)
                plt.savefig(os.path.join(out_dir, 'calibration.svg'), bbox_inches='tight')
                plt.close(fig)
            except Exception as e:
                print(f"âŒ ç»˜åˆ¶æ ¡å‡†æ›²çº¿å‡ºé”™: {e}")
            
            # Hosmer-Lemeshowæ£€éªŒ
            print(f"     è®¡ç®—Hosmer-Lemeshowæ£€éªŒ...")
            try:
                result = stats.hosmer_lemeshow_test(y_true, y_score, bins=15)
                if isinstance(result, tuple) and len(result) == 2:
                    stat, p = result
                else:
                    stat, p = np.nan, result
                
                hl_row = {
                    'Model': model_name,
                    'Task': task_name,
                    'Subset': subset,
                    'H-L Stat': f'{stat:.4f}' if not np.isnan(stat) else 'N/A',
                    'p-value': f'{p:.4f}'
                }
                pd.DataFrame([hl_row]).to_csv(os.path.join(out_dir, 'hosmer_lemeshow.csv'), index=False)
            except Exception as e:
                print(f"âŒ è®¡ç®—Hosmer-Lemeshowæ£€éªŒå‡ºé”™: {e}")
            
            # å­˜å‚¨ROCæ•°æ®ç”¨äºå¤šä»»åŠ¡å›¾
            try:
                fpr, tpr, _ = okcomp.roc_curve(y_true, y_score)
                roc_auc = okcomp.auc(fpr, tpr)
                roc_data.append({
                    'task': task_name,
                    'model': model_name,
                    'subset': subset,
                    'fpr': fpr,
                    'tpr': tpr,
                    'auc': roc_auc
                })
            except:
                pass
            
            print(f"    âœ… {subset}å­é›†å¤„ç†å®Œæˆ")
        
        # ä¿å­˜è¯¥ä»»åŠ¡çš„æ‰€æœ‰æŒ‡æ ‡
        if task_metrics:
            task_metrics_df = pd.DataFrame(task_metrics)
            task_dir = os.path.join(cfg['output_dir'], task_name, model_name)
            task_metrics_df.to_csv(os.path.join(task_dir, 'all_metrics.csv'), index=False)
    
    # 2. ç”Ÿæˆå¤šä»»åŠ¡ROCå¯¹æ¯”å›¾
    if roc_data:
        print("\nğŸ“ˆ ç”Ÿæˆå¤šä»»åŠ¡ROCå›¾...")
        summary_dir = os.path.join(cfg['output_dir'], 'summary')
        os.makedirs(summary_dir, exist_ok=True)
        
        # åˆ›å»ºå¤šä»»åŠ¡ROCå›¾
        fig = plt.figure(figsize=(14, 10))
        gs = gridspec.GridSpec(2, 2)
        axes = [fig.add_subplot(gs[i//2, i%2]) for i in range(4)]
        
        # æ”¶é›†ä»»åŠ¡æ•°æ®
        task_plots = {}
        for data in roc_data:
            task = data['task']
            if task not in task_plots:
                task_plots[task] = []
            task_plots[task].append(data)
        
        # ç»˜åˆ¶æ¯ä¸ªä»»åŠ¡çš„ROCæ›²çº¿
        for i, (task_name, task_data) in enumerate(task_plots.items()):
            if i >= 4:  # æœ€å¤šç»˜åˆ¶4ä¸ªä»»åŠ¡
                break
                
            ax = axes[i]
            for data in task_data:
                label = f"{data['model']} ({data['subset']}, AUC={data['auc']:.3f})"
                ax.plot(data['fpr'], data['tpr'], label=label)
            
            ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(f'Task: {task_name}', fontsize=12)
            ax.legend(loc="lower right", fontsize=8)
            ax.grid(True, linestyle='--', alpha=0.7)
        
        plt.suptitle('Multi-Task ROC Curves', fontsize=16, y=0.98)
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.savefig(os.path.join(summary_dir, 'multi_task_roc.svg'), bbox_inches='tight')
        plt.close(fig)
        print(f"âœ… å¤šä»»åŠ¡ROCå›¾å·²ä¿å­˜è‡³: {os.path.join(summary_dir, 'multi_task_roc.svg')}")
    
    # 3. ä¿å­˜æ±‡æ€»æŒ‡æ ‡
    if summary_metrics:
        summary_dir = os.path.join(cfg['output_dir'], 'summary')
        os.makedirs(summary_dir, exist_ok=True)
        
        summary_df = pd.DataFrame(summary_metrics)
        summary_file = os.path.join(summary_dir, 'multi_task_metrics.csv')
        summary_df.to_csv(summary_file, index=False)
        print(f"âœ… æ±‡æ€»æŒ‡æ ‡å·²ä¿å­˜è‡³: {summary_file}")
    
    print('\nğŸ‰ å…¨éƒ¨å®Œæˆï¼')

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        sys.exit(1)
